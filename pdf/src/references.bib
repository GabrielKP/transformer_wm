
@article{merity_pointer_2016,
	title = {Pointer {Sentinel} {Mixture} {Models}},
	url = {https://arxiv.org/abs/1609.07843v1},
	doi = {10.48550/arXiv.1609.07843},
	abstract = {Recent neural network sequence models with softmax classifiers have achieved their best language modeling performance only with very large hidden states and large vocabularies. Even then they struggle to predict rare or unseen words even if the context makes the prediction unambiguous. We introduce the pointer sentinel mixture architecture for neural sequence models which has the ability to either reproduce a word from the recent context or produce a word from a standard softmax classifier. Our pointer sentinel-LSTM model achieves state of the art language modeling performance on the Penn Treebank (70.9 perplexity) while using far fewer parameters than a standard softmax LSTM. In order to evaluate how well language models can exploit longer contexts and deal with more realistic vocabularies and larger corpora we also introduce the freely available WikiText corpus.},
	language = {en},
	urldate = {2022-06-23},
	author = {Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
	month = sep,
	year = {2016},
}

@inproceedings{yogatama_memory_2022,
	title = {Memory {Architectures} in {Recurrent} {Neural} {Network} {Language} {Models}},
	url = {https://openreview.net/forum?id=SkFqf0lAZ},
	abstract = {We compare and analyze sequential, random access, and stack memory architectures for recurrent neural network language models. Our experiments on the Penn Treebank and Wikitext-2 datasets show that...},
	language = {en},
	urldate = {2022-06-22},
	author = {Yogatama, Dani and Miao, Yishu and Melis, Gabor and Ling, Wang and Kuncoro, Adhiguna and Dyer, Chris and Blunsom, Phil},
	month = feb,
	year = {2022},
}

@misc{dai_transformer-xl_2019,
	title = {Transformer-{XL}: {Attentive} {Language} {Models} {Beyond} a {Fixed}-{Length} {Context}},
	shorttitle = {Transformer-{XL}},
	url = {http://arxiv.org/abs/1901.02860},
	doi = {10.48550/arXiv.1901.02860},
	abstract = {Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80\% longer than RNNs and 450\% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.},
	urldate = {2022-06-22},
	collaborator = {Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc V. and Salakhutdinov, Ruslan},
	month = jun,
	year = {2019},
	note = {Number: arXiv:1901.02860
arXiv:1901.02860 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{paszke_pytorch_2019,
	title = {{PyTorch}: {An} {Imperative} {Style}, {High}-{Performance} {Deep} {Learning} {Library}},
	abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientiﬁc computing libraries, while remaining efﬁcient and supporting hardware accelerators such as GPUs.},
	language = {en},
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	year = {2019},
	pages = {12},
}

@misc{lillicrap_what_2019,
	title = {What does it mean to understand a neural network?},
	url = {http://arxiv.org/abs/1907.06374},
	doi = {10.48550/arXiv.1907.06374},
	abstract = {We can define a neural network that can learn to recognize objects in less than 100 lines of code. However, after training, it is characterized by millions of weights that contain the knowledge about many object types across visual scenes. Such networks are thus dramatically easier to understand in terms of the code that makes them than the resulting properties, such as tuning or connections. In analogy, we conjecture that rules for development and learning in brains may be far easier to understand than their resulting properties. The analogy suggests that neuroscience would benefit from a focus on learning and development.},
	urldate = {2022-06-22},
	collaborator = {Lillicrap, Timothy P. and Kording, Konrad P.},
	month = jul,
	year = {2019},
	note = {Number: arXiv:1907.06374
arXiv:1907.06374 [cs, q-bio, stat]},
	keywords = {Computer Science - Machine Learning, Quantitative Biology - Neurons and Cognition, Statistics - Machine Learning},
}

@techreport{tay_long_2020,
	title = {Long {Range} {Arena}: {A} {Benchmark} for {Efficient} {Transformers}},
	shorttitle = {Long {Range} {Arena}},
	url = {http://arxiv.org/abs/2011.04006},
	abstract = {Transformers do not scale very well to long sequence lengths largely because of quadratic self-attention complexity. In the recent months, a wide spectrum of efficient, fast Transformers have been proposed to tackle this problem, more often than not claiming superior or comparable model quality to vanilla Transformer models. To this date, there is no well-established consensus on how to evaluate this class of models. Moreover, inconsistent benchmarking on a wide spectrum of tasks and datasets makes it difficult to assess relative model quality amongst many models. This paper proposes a systematic and unified benchmark, LRA, specifically focused on evaluating model quality under long-context scenarios. Our benchmark is a suite of tasks consisting of sequences ranging from \$1K\$ to \$16K\$ tokens, encompassing a wide range of data types and modalities such as text, natural, synthetic images, and mathematical expressions requiring similarity, structural, and visual-spatial reasoning. We systematically evaluate ten well-established long-range Transformer models (Reformers, Linformers, Linear Transformers, Sinkhorn Transformers, Performers, Synthesizers, Sparse Transformers, and Longformers) on our newly proposed benchmark suite. LRA paves the way towards better understanding this class of efficient Transformer models, facilitates more research in this direction, and presents new challenging tasks to tackle. Our benchmark code will be released at https://github.com/google-research/long-range-arena.},
	number = {arXiv:2011.04006},
	urldate = {2022-05-27},
	institution = {arXiv},
	author = {Tay, Yi and Dehghani, Mostafa and Abnar, Samira and Shen, Yikang and Bahri, Dara and Pham, Philip and Rao, Jinfeng and Yang, Liu and Ruder, Sebastian and Metzler, Donald},
	month = nov,
	year = {2020},
	doi = {10.48550/arXiv.2011.04006},
	note = {arXiv:2011.04006 [cs]
type: article},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Information Retrieval, Computer Science - Machine Learning},
}

@inproceedings{pavlick_ppdb_2015,
	address = {Beijing, China},
	title = {{PPDB} 2.0: {Better} paraphrase ranking, fine-grained entailment relations, word embeddings, and style classification},
	url = {https://aclanthology.org/P15-2070},
	doi = {10.3115/v1/P15-2070},
	booktitle = {Proceedings of the 53rd {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 7th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 2: {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Pavlick, Ellie and Rastogi, Pushpendre and Ganitkevitch, Juri and Van Durme, Benjamin and Callison-Burch, Chris},
	month = jul,
	year = {2015},
	pages = {425--430},
}

@article{mccloskey_networks_1991,
	title = {Networks and {Theories}: {The} {Place} of {Connectionism} in {Cognitive} {Science}},
	volume = {2},
	issn = {0956-7976},
	shorttitle = {Networks and {Theories}},
	url = {https://doi.org/10.1111/j.1467-9280.1991.tb00173.x},
	doi = {10.1111/j.1467-9280.1991.tb00173.x},
	abstract = {This article considers how connectionist modeling can contribute to understanding of human cognition. J argue that connectionist networks should not be thought of as theories or simulations of theories, but may nevertheless contribute to the development of theories.},
	language = {en},
	number = {6},
	urldate = {2022-06-20},
	journal = {Psychological Science},
	author = {McCloskey, Michael},
	month = nov,
	year = {1991},
	note = {Publisher: SAGE Publications Inc},
	pages = {387--395},
}

@inproceedings{dagan_pascal_2006,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {The {PASCAL} {Recognising} {Textual} {Entailment} {Challenge}},
	isbn = {978-3-540-33428-6},
	doi = {10.1007/11736790_9},
	abstract = {This paper describes the PASCAL Network of Excellence first Recognising Textual Entailment (RTE-1) Challenge benchmark. The RTE task is defined as recognizing, given two text fragments, whether the meaning of one text can be inferred (entailed) from the other. This application-independent task is suggested as capturing major inferences about the variability of semantic expression which are commonly needed across multiple applications. The Challenge has raised noticeable attention in the research community, attracting 17 submissions from diverse groups, suggesting the generic relevance of the task.},
	language = {en},
	booktitle = {Machine {Learning} {Challenges}. {Evaluating} {Predictive} {Uncertainty}, {Visual} {Object} {Classification}, and {Recognising} {Tectual} {Entailment}},
	publisher = {Springer},
	author = {Dagan, Ido and Glickman, Oren and Magnini, Bernardo},
	editor = {Quiñonero-Candela, Joaquin and Dagan, Ido and Magnini, Bernardo and d’Alché-Buc, Florence},
	year = {2006},
	keywords = {Information Extraction, Machine Translation, News Story, Question Answering, Reading Comprehension},
	pages = {177--190},
}

@misc{noauthor_frontiers_nodate,
	title = {Frontiers {\textbar} {Goal} {Commitments} and the content of thoughts and dreams: basic principles {\textbar} {Psychology}},
	url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2013.00415/full},
	urldate = {2022-06-15},
}

@misc{noauthor_neural_nodate,
	title = {A neural network model of when to retrieve and encode episodic memories {\textbar} {eLife}},
	url = {https://elifesciences.org/articles/74445},
	urldate = {2022-06-15},
}

@book{marr_vision_1982,
	address = {Cambridge, MA, USA},
	title = {Vision: {A} {Computational} {Investigation} into the {Human} {Representation} and {Processing} of {Visual} {Information}},
	isbn = {0-7167-1284-9},
	shorttitle = {Vision},
	language = {en},
	publisher = {W.H.Freeman \& Co Ltd},
	author = {Marr, David},
	year = {1982},
}

@article{allen_raincloud_2019,
	title = {Raincloud plots: a multi-platform tool for robust data visualization},
	volume = {4},
	issn = {2398-502X},
	shorttitle = {Raincloud plots},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6480976/},
	doi = {10.12688/wellcomeopenres.15191.1},
	abstract = {Across scientific disciplines, there is a rapidly growing recognition of the need for more statistically robust, transparent approaches to data visualization. Complementary to this, many scientists have called for plotting tools that accurately and transparently convey key aspects of statistical effects and raw data with minimal distortion. Previously common approaches, such as plotting conditional mean or median barplots together with error-bars have been criticized for distorting effect size, hiding underlying patterns in the raw data, and obscuring the assumptions upon which the most commonly used statistical tests are based. Here we describe a data visualization approach which overcomes these issues, providing maximal statistical information while preserving the desired ‘inference at a glance’ nature of barplots and other similar visualization devices. These “raincloud plots” can visualize raw data, probability density, and key summary statistics such as median, mean, and relevant confidence intervals in an appealing and flexible format with minimal redundancy. In this tutorial paper, we provide basic demonstrations of the strength of raincloud plots and similar approaches, outline potential modifications for their optimal use, and provide open-source code for their streamlined implementation in R, Python and Matlab (
https://github.com/RainCloudPlots/RainCloudPlots). Readers can investigate the R and Python tutorials interactively in the browser using Binder by Project Jupyter.},
	urldate = {2022-06-09},
	journal = {Wellcome Open Research},
	author = {Allen, Micah and Poggiali, Davide and Whitaker, Kirstie and Marshall, Tom Rhys and Kievit, Rogier A.},
	month = apr,
	year = {2019},
	pmid = {31069261},
	pmcid = {PMC6480976},
	pages = {63},
}

@techreport{van_nguyen_trankit_2021,
	title = {Trankit: {A} {Light}-{Weight} {Transformer}-based {Toolkit} for {Multilingual} {Natural} {Language} {Processing}},
	shorttitle = {Trankit},
	url = {http://arxiv.org/abs/2101.03289},
	abstract = {We introduce Trankit, a light-weight Transformer-based Toolkit for multilingual Natural Language Processing (NLP). It provides a trainable pipeline for fundamental NLP tasks over 100 languages, and 90 pretrained pipelines for 56 languages. Built on a state-of-the-art pretrained language model, Trankit significantly outperforms prior multilingual NLP pipelines over sentence segmentation, part-of-speech tagging, morphological feature tagging, and dependency parsing while maintaining competitive performance for tokenization, multi-word token expansion, and lemmatization over 90 Universal Dependencies treebanks. Despite the use of a large pretrained transformer, our toolkit is still efficient in memory usage and speed. This is achieved by our novel plug-and-play mechanism with Adapters where a multilingual pretrained transformer is shared across pipelines for different languages. Our toolkit along with pretrained models and code are publicly available at: https://github.com/nlp-uoregon/trankit. A demo website for our toolkit is also available at: http://nlp.uoregon.edu/trankit. Finally, we create a demo video for Trankit at: https://youtu.be/q0KGP3zGjGc.},
	number = {arXiv:2101.03289},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Van Nguyen, Minh and Lai, Viet Dac and Veyseh, Amir Pouran Ben and Nguyen, Thien Huu},
	month = oct,
	year = {2021},
	doi = {10.48550/arXiv.2101.03289},
	note = {arXiv:2101.03289 [cs]
type: article},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{yuan_sornet_2021,
	title = {{SORNet}: {Spatial} {Object}-{Centric} {Representations} for {Sequential} {Manipulation}},
	shorttitle = {{SORNet}},
	url = {https://openreview.net/forum?id=mOLu2rODIJF},
	abstract = {Sequential manipulation tasks require a robot to perceive the state of an environment and plan a sequence of actions leading to a desired goal state, where the ability to reason about spatial...},
	language = {en},
	urldate = {2022-06-08},
	author = {Yuan, Wentao and Paxton, Chris and Desingh, Karthik and Fox, Dieter},
	month = jun,
	year = {2021},
}

@article{miller_wordnet_1995,
	title = {{WordNet}: a lexical database for {English}},
	volume = {38},
	issn = {0001-0782},
	shorttitle = {{WordNet}},
	url = {https://doi.org/10.1145/219717.219748},
	doi = {10.1145/219717.219748},
	abstract = {Because meaningful sentences are composed of meaningful words, any system that hopes to process natural languages as people do must have information about words and their meanings. This information is traditionally provided through dictionaries, and machine-readable dictionaries are now widely available. But dictionary entries evolved for the convenience of human readers, not for machines. WordNet1 provides a more effective combination of traditional lexicographic information and modern computing. WordNet is an online lexical database designed for use under program control. English nouns, verbs, adjectives, and adverbs are organized into sets of synonyms, each representing a lexicalized concept. Semantic relations link the synonym sets [4].},
	number = {11},
	urldate = {2022-06-06},
	journal = {Communications of the ACM},
	author = {Miller, George A.},
	month = nov,
	year = {1995},
	pages = {39--41},
}

@techreport{tay_efficient_2022,
	title = {Efficient {Transformers}: {A} {Survey}},
	shorttitle = {Efficient {Transformers}},
	url = {http://arxiv.org/abs/2009.06732},
	abstract = {Transformer model architectures have garnered immense interest lately due to their effectiveness across a range of domains like language, vision and reinforcement learning. In the field of natural language processing for example, Transformers have become an indispensable staple in the modern deep learning stack. Recently, a dizzying number of "X-former" models have been proposed - Reformer, Linformer, Performer, Longformer, to name a few - which improve upon the original Transformer architecture, many of which make improvements around computational and memory efficiency. With the aim of helping the avid researcher navigate this flurry, this paper characterizes a large and thoughtful selection of recent efficiency-flavored "X-former" models, providing an organized and comprehensive overview of existing work and models across multiple domains.},
	number = {arXiv:2009.06732},
	urldate = {2022-05-26},
	institution = {arXiv},
	author = {Tay, Yi and Dehghani, Mostafa and Bahri, Dara and Metzler, Donald},
	month = mar,
	year = {2022},
	doi = {10.48550/arXiv.2009.06732},
	note = {arXiv:2009.06732 [cs]
type: article},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Information Retrieval, Computer Science - Machine Learning},
}

@article{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	url = {http://arxiv.org/abs/2005.14165},
	abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
	urldate = {2022-04-21},
	journal = {arXiv:2005.14165 [cs]},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	month = jul,
	year = {2020},
	note = {arXiv: 2005.14165},
	keywords = {Computer Science - Computation and Language, GPT3},
}

@article{radford_language_2019,
	title = {Language {Models} are {Unsupervised} {Multitask} {Learners}},
	abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspeciﬁc datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underﬁts WebText. Samples from the model reﬂect these improvements and contain coherent paragraphs of text. These ﬁndings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
	language = {en},
	author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
	year = {2019},
	keywords = {GPT2},
	pages = {24},
}

@article{radford_improving_2018,
	title = {Improving {Language} {Understanding} by {Generative} {Pre}-{Training}},
	abstract = {Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classiﬁcation. Although large unlabeled text corpora are abundant, labeled data for learning these speciﬁc tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative ﬁne-tuning on each speciﬁc task. In contrast to previous approaches, we make use of task-aware input transformations during ﬁne-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures speciﬁcally crafted for each task, signiﬁcantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9\% on commonsense reasoning (Stories Cloze Test), 5.7\% on question answering (RACE), and 1.5\% on textual entailment (MultiNLI).},
	language = {en},
	author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
	year = {2018},
	keywords = {GPT},
	pages = {12},
}

@article{elman_finding_1990,
	title = {Finding {Structure} in {Time}},
	volume = {14},
	issn = {1551-6709},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog1402_1},
	doi = {10.1207/s15516709cog1402_1},
	abstract = {Time underlies many interesting human behaviors. Thus, the question of how to represent time in connectionist models is very important. One approach is to represent time implicitly by its effects on processing rather than explicitly (as in a spatial representation). The current report develops a proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory. In this approach, hidden unit patterns are fed back to themselves: the internal representations which develop thus reflect task demands in the context of prior internal states. A set of simulations is reported which range from relatively simple problems (temporal version of XOR) to discovering syntactic/semantic features for words. The networks are able to learn interesting internal representations which incorporate task demands with memory demands: indeed, in this approach the notion of memory is inextricably bound up with task processing. These representations reveal a rich structure, which allows them to be highly context-dependent, while also expressing generalizations across classes of items. These representations suggest a method for representing lexical categories and the type/token distinction.},
	language = {en},
	number = {2},
	urldate = {2022-05-25},
	journal = {Cognitive Science},
	author = {Elman, Jeffrey L.},
	year = {1990},
	pages = {179--211},
}

@article{vaswani_attention_2017,
	title = {Attention is {All} you {Need}},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
	language = {en},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
	year = {2017},
	pages = {11},
}

@techreport{sankar_neural_2019,
	title = {Do {Neural} {Dialog} {Systems} {Use} the {Conversation} {History} {Effectively}? {An} {Empirical} {Study}},
	shorttitle = {Do {Neural} {Dialog} {Systems} {Use} the {Conversation} {History} {Effectively}?},
	url = {http://arxiv.org/abs/1906.01603},
	abstract = {Neural generative models have been become increasingly popular when building conversational agents. They offer ﬂexibility, can be easily adapted to new domains, and require minimal domain engineering. A common criticism of these systems is that they seldom understand or use the available dialog history effectively. In this paper, we take an empirical approach to understanding how these models use the available dialog history by studying the sensitivity of the models to artiﬁcially introduced unnatural changes or perturbations to their context at test time. We experiment with 10 different types of perturbations on 4 multi-turn dialog datasets and ﬁnd that commonly used neural dialog architectures like recurrent and transformer-based seq2seq models are rarely sensitive to most perturbations such as missing or reordering utterances, shufﬂing words, etc. Also, by open-sourcing our code, we believe that it will serve as a useful diagnostic tool for evaluating dialog systems in the future 1.},
	language = {en},
	number = {arXiv:1906.01603},
	urldate = {2022-05-25},
	institution = {arXiv},
	author = {Sankar, Chinnadhurai and Subramanian, Sandeep and Pal, Christopher and Chandar, Sarath and Bengio, Yoshua},
	month = jul,
	year = {2019},
	note = {arXiv:1906.01603 [cs]
type: article},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@techreport{chelba_n-gram_2017,
	title = {N-gram {Language} {Modeling} using {Recurrent} {Neural} {Network} {Estimation}},
	url = {http://arxiv.org/abs/1703.10724},
	abstract = {We investigate the effective memory depth of RNN models by using them for \$n\$-gram language model (LM) smoothing. Experiments on a small corpus (UPenn Treebank, one million words of training data and 10k vocabulary) have found the LSTM cell with dropout to be the best model for encoding the \$n\$-gram state when compared with feed-forward and vanilla RNN models. When preserving the sentence independence assumption the LSTM \$n\$-gram matches the LSTM LM performance for \$n=9\$ and slightly outperforms it for \$n=13\$. When allowing dependencies across sentence boundaries, the LSTM \$13\$-gram almost matches the perplexity of the unlimited history LSTM LM. LSTM \$n\$-gram smoothing also has the desirable property of improving with increasing \$n\$-gram order, unlike the Katz or Kneser-Ney back-off estimators. Using multinomial distributions as targets in training instead of the usual one-hot target is only slightly beneficial for low \$n\$-gram orders. Experiments on the One Billion Words benchmark show that the results hold at larger scale: while LSTM smoothing for short \$n\$-gram contexts does not provide significant advantages over classic N-gram models, it becomes effective with long contexts (\$n {\textgreater} 5\$); depending on the task and amount of data it can match fully recurrent LSTM models at about \$n=13\$. This may have implications when modeling short-format text, e.g. voice search/query LMs. Building LSTM \$n\$-gram LMs may be appealing for some practical situations: the state in a \$n\$-gram LM can be succinctly represented with \$(n-1)*4\$ bytes storing the identity of the words in the context and batches of \$n\$-gram contexts can be processed in parallel. On the downside, the \$n\$-gram context encoding computed by the LSTM is discarded, making the model more expensive than a regular recurrent LSTM LM.},
	number = {arXiv:1703.10724},
	urldate = {2022-05-25},
	institution = {arXiv},
	author = {Chelba, Ciprian and Norouzi, Mohammad and Bengio, Samy},
	month = jun,
	year = {2017},
	doi = {10.48550/arXiv.1703.10724},
	note = {arXiv:1703.10724 [cs]
type: article},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{mikolov_recurrent_2010,
	title = {Recurrent neural network based language model},
	volume = {2},
	abstract = {A new recurrent neural network based language model (RNN LM) with applications to speech recognition is presented. Results indicate that it is possible to obtain around 50\% reduction of perplexity by using mixture of several RNN LMs, compared to a state of the art backoff language model. Speech recognition experiments show around 18\% reduction of word error rate on the Wall Street Journal task when comparing models trained on the same amount of data, and around 5\% on the much harder NIST RT05 task, even when the backoff model is trained on much more data than the RNN LM. We provide ample empirical evidence to suggest that connectionist language models are superior to standard n-gram techniques, except their high computational (training) complexity.},
	author = {Mikolov, Tomas and Karafiát, Martin and Burget, Lukas and Cernocký, Jan and Khudanpur, Sanjeev},
	month = jan,
	year = {2010},
	pages = {1045--1048},
}

@article{hochreiter_long_1997,
	title = {Long {Short}-{Term} {Memory}},
	volume = {9},
	issn = {0899-7667},
	doi = {10.1162/neco.1997.9.8.1735},
	abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
	number = {8},
	journal = {Neural Computation},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	month = nov,
	year = {1997},
	note = {Conference Name: Neural Computation},
	pages = {1735--1780},
}

@misc{gokaslan_openwebtext_2019,
	title = {{OpenWebText} {Corpus}},
	url = {https://skylion007.github.io/OpenWebTextCorpus/},
	abstract = {Home of the Open WebText Corpus.},
	language = {en},
	urldate = {2022-05-23},
	journal = {OpenWebTextCorpus},
	author = {Gokaslan, Aaron and Cohen, Vanya},
	year = {2019},
}

@misc{noauthor_distilgpt2_nodate,
	title = {distilgpt2 · {Hugging} {Face}},
	url = {https://huggingface.co/distilgpt2},
	abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2022-05-23},
}

@techreport{sanh_distilbert_2020,
	title = {{DistilBERT}, a distilled version of {BERT}: smaller, faster, cheaper and lighter},
	shorttitle = {{DistilBERT}, a distilled version of {BERT}},
	url = {http://arxiv.org/abs/1910.01108},
	abstract = {As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40\%, while retaining 97\% of its language understanding capabilities and being 60\% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.},
	number = {arXiv:1910.01108},
	urldate = {2022-05-23},
	institution = {arXiv},
	author = {Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
	month = feb,
	year = {2020},
	doi = {10.48550/arXiv.1910.01108},
	note = {arXiv:1910.01108 [cs]
type: article},
	keywords = {Computer Science - Computation and Language},
}

@article{li_understanding_2017,
	title = {Understanding {Neural} {Networks} through {Representation} {Erasure}},
	url = {http://arxiv.org/abs/1612.08220},
	abstract = {While neural networks have been successfully applied to many natural language processing tasks, they come at the cost of interpretability. In this paper, we propose a general methodology to analyze and interpret decisions from a neural model by observing the effects on the model of erasing various parts of the representation, such as input word-vector dimensions, intermediate hidden units, or input words. We present several approaches to analyzing the effects of such erasure, from computing its impact on evaluation metrics, to using reinforcement learning to erase the minimum set of input words in order to ﬂip a neural model’s decision. In a comprehensive analysis of multiple NLP tasks from lexical (word shape, morphology) to sentence-level (sentiment) to document level (sentiment aspect), we show that the proposed methodology not only offers clear explanations about neural model decisions, but also provides a way to conduct error analysis on neural models.},
	language = {en},
	urldate = {2022-03-05},
	journal = {arXiv:1612.08220 [cs]},
	author = {Li, Jiwei and Monroe, Will and Jurafsky, Dan},
	month = jan,
	year = {2017},
	note = {arXiv: 1612.08220},
	keywords = {Computer Science - Computation and Language, READ},
}

@article{firestone_performance_2020,
	title = {Performance vs. competence in human–machine comparisons},
	volume = {117},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1905334117},
	doi = {10.1073/pnas.1905334117},
	abstract = {Does the human mind resemble the machines that can behave like it? Biologically inspired machine-learning systems approach “human-level” accuracy in an astounding variety of domains, and even predict human brain activity—raising the exciting possibility that such systems represent the world like we do. However, even seemingly intelligent machines
              fail
              in strange and “unhumanlike” ways, threatening their status as models of our minds. How can we know when human–machine behavioral differences reflect deep disparities in their underlying capacities, vs. when such failures are only superficial or peripheral? This article draws on a foundational insight from cognitive science—the distinction between
              performance
              and
              competence
              —to encourage “species-fair” comparisons between humans and machines. The performance/competence distinction urges us to consider whether the failure of a system to behave as ideally hypothesized, or the failure of one creature to behave like another, arises not because the system lacks the relevant knowledge or internal capacities (“competence”), but instead because of superficial constraints on demonstrating that knowledge (“performance”). I argue that this distinction has been neglected by research comparing human and machine behavior, and that it should be essential to any such comparison. Focusing on the domain of image classification, I identify three factors contributing to the species-fairness of human–machine comparisons, extracted from recent work that equates such constraints. Species-fair comparisons level the playing field between natural and artificial intelligence, so that we can separate more superficial differences from those that may be deep and enduring.},
	language = {en},
	number = {43},
	urldate = {2022-02-04},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Firestone, Chaz},
	month = oct,
	year = {2020},
	pages = {26562--26571},
}

@article{herz_overarching_2020,
	title = {Overarching {States} of {Mind}},
	volume = {24},
	issn = {1364-6613},
	url = {https://www.sciencedirect.com/science/article/pii/S1364661320300012},
	doi = {10.1016/j.tics.2019.12.015},
	abstract = {We all have our varying mental emphases, inclinations, and biases. These individual dispositions are dynamic in that they can change over time and context. We propose that these changing states of mind (SoMs) are holistic in that they exert all-encompassing and coordinated effects simultaneously on our perception, attention, thought, affect, and behavior. Given the breadth of their reach, understanding how SoMs operate is essential. We provide evidence and a framework for the concept of SoM, and we propose a unifying principle for the underlying cortical mechanism whereby SoM is determined by the balance between top-down (TD) and bottom-up (BU) processing. This novel global account gives rise to unique hypotheses and opens new horizons for understanding the human mind.},
	language = {en},
	number = {3},
	urldate = {2022-02-02},
	journal = {Trends in Cognitive Sciences},
	author = {Herz, Noa and Baror, Shira and Bar, Moshe},
	month = mar,
	year = {2020},
	keywords = {bottom-up, context, creativity, exploitation, exploration, mindset, mood, top-down},
	pages = {184--199},
}

@article{gardner_maps_2008,
	title = {Maps of {Visual} {Space} in {Human} {Occipital} {Cortex} {Are} {Retinotopic}, {Not} {Spatiotopic}},
	volume = {28},
	issn = {0270-6474, 1529-2401},
	url = {https://www.jneurosci.org/lookup/doi/10.1523/JNEUROSCI.5476-07.2008},
	doi = {10.1523/JNEUROSCI.5476-07.2008},
	language = {en},
	number = {15},
	urldate = {2022-01-29},
	journal = {Journal of Neuroscience},
	author = {Gardner, J. L. and Merriam, E. P. and Movshon, J. A. and Heeger, D. J.},
	month = apr,
	year = {2008},
	pages = {3988--3999},
}

@article{wei_frequency_2021,
	title = {Frequency {Effects} on {Syntactic} {Rule} {Learning} in {Transformers}},
	url = {http://arxiv.org/abs/2109.07020},
	abstract = {Pre-trained language models perform well on a variety of linguistic tasks that require symbolic reasoning, raising the question of whether such models implicitly represent abstract symbols and rules. We investigate this question using the case study of BERT’s performance on English subject–verb agreement. Unlike prior work, we train multiple instances of BERT from scratch, allowing us to perform a series of controlled interventions at pre-training time. We show that BERT often generalizes well to subject–verb pairs that never occurred in training, suggesting a degree of rule-governed behavior. We also ﬁnd, however, that performance is heavily inﬂuenced by word frequency, with experiments showing that both the absolute frequency of a verb form, as well as the frequency relative to the alternate inﬂection, are causally implicated in the predictions BERT makes at inference time. Closer analysis of these frequency effects reveals that BERT’s behavior is consistent with a system that correctly applies the SVA rule in general but struggles to overcome strong training priors and to estimate agreement features (singular vs. plural) on infrequent lexical items.},
	language = {en},
	urldate = {2021-12-18},
	journal = {arXiv:2109.07020 [cs]},
	author = {Wei, Jason and Garrette, Dan and Linzen, Tal and Pavlick, Ellie},
	month = sep,
	year = {2021},
	note = {arXiv: 2109.07020},
	keywords = {Computer Science - Computation and Language},
}

@article{dohmatob_dark_2020,
	title = {Dark control: {The} default mode network as a reinforcement learning agent},
	volume = {41},
	issn = {1097-0193},
	shorttitle = {Dark control},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/hbm.25019},
	doi = {10.1002/hbm.25019},
	abstract = {The default mode network (DMN) is believed to subserve the baseline mental activity in humans. Its higher energy consumption compared to other brain networks and its intimate coupling with conscious awareness are both pointing to an unknown overarching function. Many research streams speak in favor of an evolutionarily adaptive role in envisioning experience to anticipate the future. In the present work, we propose a process model that tries to explain how the DMN may implement continuous evaluation and prediction of the environment to guide behavior. The main purpose of DMN activity, we argue, may be described by Markov decision processes that optimize action policies via value estimates through vicarious trial and error. Our formal perspective on DMN function naturally accommodates as special cases previous interpretations based on (a) predictive coding, (b) semantic associations, and (c) a sentinel role. Moreover, this process model for the neural optimization of complex behavior in the DMN offers parsimonious explanations for recent experimental findings in animals and humans.},
	language = {en},
	number = {12},
	urldate = {2022-02-03},
	journal = {Human Brain Mapping},
	author = {Dohmatob, Elvis and Dumas, Guillaume and Bzdok, Danilo},
	year = {2020},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/hbm.25019},
	keywords = {READ, artificial intelligence, human intelligence, systems neuroscience},
	pages = {3318--3341},
}

@article{tian_information_2021,
	title = {Information {Evolution} in {Complex} {Networks}},
	url = {http://arxiv.org/abs/2111.06608},
	abstract = {Many biological phenomena or social events critically depend on how information evolves in complex networks. A seeming paradox of the information evolution is the coexistence of local randomness, manifested as the stochastic distortion of information content during individual-individual diffusion, and global regularity, illustrated by speciﬁc non-random patterns of information content on the network scale. The current research pursues to understand the underlying mechanisms of such coexistence. Applying network dynamics and information theory, we discover that a certain amount of information, determined by the selectivity of networks to the input information, frequently survives from random distortion. Other information will inevitably experience distortion or dissipation, whose speeds are shaped by the diversity of information selectivity in networks. The discovered laws exist irrespective of noise, but the noise accounts for their intensiﬁcation. We further demonstrate the ubiquity of our discovered laws by applying them to analyze the emergence of neural tuning properties in the primary visual and medial temporal cortices of animal brains and the emergence of extreme opinions in social networks.},
	language = {en},
	urldate = {2022-01-29},
	journal = {arXiv:2111.06608 [cond-mat, physics:nlin, physics:physics, q-bio]},
	author = {Tian, Yang and Gardner, Justin L. and Li, Guoqi and Sun, Pei},
	month = nov,
	year = {2021},
	note = {arXiv: 2111.06608},
	keywords = {Condensed Matter - Disordered Systems and Neural Networks, Nonlinear Sciences - Pattern Formation and Solitons, Physics - Biological Physics, Quantitative Biology - Neurons and Cognition},
}

@article{yang_xlnet_2020,
	title = {{XLNet}: {Generalized} {Autoregressive} {Pretraining} for {Language} {Understanding}},
	shorttitle = {{XLNet}},
	url = {http://arxiv.org/abs/1906.08237},
	abstract = {With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.},
	urldate = {2022-04-21},
	journal = {arXiv:1906.08237 [cs]},
	author = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Ruslan and Le, Quoc V.},
	month = jan,
	year = {2020},
	note = {arXiv: 1906.08237},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{wolf_transformers_2020,
	address = {Online},
	title = {Transformers: {State}-of-the-{Art} {Natural} {Language} {Processing}},
	shorttitle = {Transformers},
	url = {https://www.aclweb.org/anthology/2020.emnlp-demos.6},
	doi = {10.18653/v1/2020.emnlp-demos.6},
	abstract = {Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered stateof-the art Transformer architectures under a uniﬁed API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at https://github.com/ huggingface/transformers.},
	language = {en},
	urldate = {2022-04-21},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}: {System} {Demonstrations}},
	publisher = {Association for Computational Linguistics},
	author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, Remi and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and von Platen, Patrick and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Le Scao, Teven and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander},
	year = {2020},
	pages = {38--45},
}

@article{armeni_assessing_nodate,
	title = {Assessing verbatim short-term memory in neural language models},
	journal = {2022},
	author = {Armeni, Kristijan and Honey, Christopher J. and Linzen, Tal},
}

@article{ravishankar_word_2022,
	title = {Word {Order} {Does} {Matter} ({And} {Shuffled} {Language} {Models} {Know} {It})},
	url = {http://arxiv.org/abs/2203.10995},
	abstract = {Recent studies have shown that language models pretrained and/or fine-tuned on randomly permuted sentences exhibit competitive performance on GLUE, putting into question the importance of word order information. Somewhat counter-intuitively, some of these studies also report that position embeddings appear to be crucial for models' good performance with shuffled text. We probe these language models for word order information and investigate what position embeddings learned from shuffled text encode, showing that these models retain information pertaining to the original, naturalistic word order. We show this is in part due to a subtlety in how shuffling is implemented in previous work -- before rather than after subword segmentation. Surprisingly, we find even Language models trained on text shuffled after subword segmentation retain some semblance of information about word order because of the statistical dependencies between sentence length and unigram probabilities. Finally, we show that beyond GLUE, a variety of language understanding tasks do require word order information, often to an extent that cannot be learned through fine-tuning.},
	urldate = {2022-03-25},
	journal = {arXiv:2203.10995 [cs]},
	author = {Ravishankar, Vinit and Abdou, Mostafa and Kulmizev, Artur and Søgaard, Anders},
	month = mar,
	year = {2022},
	note = {arXiv: 2203.10995},
	keywords = {Computer Science - Computation and Language},
}

@article{zhang_paws_nodate,
	title = {{PAWS}: {Paraphrase} {Adversaries} from {Word} {Scrambling}},
	abstract = {Existing paraphrase identiﬁcation datasets lack sentence pairs that have high lexical overlap without being paraphrases. Models trained on such data fail to distinguish pairs like ﬂights from New York to Florida and ﬂights from Florida to New York. This paper introduces PAWS (Paraphrase Adversaries from Word Scrambling), a new dataset with 108,463 wellformed paraphrase and non-paraphrase pairs with high lexical overlap. Challenging pairs are generated by controlled word swapping and back translation, followed by ﬂuency and paraphrase judgments by human raters. Stateof-the-art models trained on existing datasets have dismal performance on PAWS ({\textless}40\% accuracy); however, including PAWS training data for these models improves their accuracy to 85\% while maintaining performance on existing tasks. In contrast, models that do not capture non-local contextual information fail even with PAWS training examples. As such, PAWS provides an effective instrument for driving further progress on models that better exploit structure, context, and pairwise comparisons.},
	language = {en},
	author = {Zhang, Yuan and Baldridge, Jason and He, Luheng},
	keywords = {READ},
	pages = {11},
}

@article{pham_out_2021,
	title = {Out of {Order}: {How} {Important} {Is} {The} {Sequential} {Order} of {Words} in a {Sentence} in {Natural} {Language} {Understanding} {Tasks}?},
	shorttitle = {Out of {Order}},
	url = {http://arxiv.org/abs/2012.15180},
	abstract = {Do state-of-the-art natural language understanding models care about word order? Not always! We found 75\% to 90\% of the correct predictions of BERT-based classiﬁers, trained on many GLUE tasks, remain constant after input words are randomly shufﬂed. Although BERT embeddings are famously contextual, the contribution of each individual word to classiﬁcation is almost unchanged even after its surrounding words are shufﬂed. BERTbased models exploit superﬁcial cues (e.g. the sentiment of keywords in sentiment analysis; or the word-wise similarity between sequencepair inputs in natural language inference) to make correct decisions when tokens are randomly shufﬂed. Encouraging models to capture word order information improves the performance on most GLUE tasks and SQuAD 2.0. Our work suggests that many GLUE tasks are not challenging machines to understand the meaning of a sentence.},
	language = {en},
	urldate = {2022-03-06},
	journal = {arXiv:2012.15180 [cs]},
	author = {Pham, Thang M. and Bui, Trung and Mai, Long and Nguyen, Anh},
	month = jul,
	year = {2021},
	note = {arXiv: 2012.15180},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{liu_roberta_2019,
	title = {{RoBERTa}: {A} {Robustly} {Optimized} {BERT} {Pretraining} {Approach}},
	shorttitle = {{RoBERTa}},
	url = {http://arxiv.org/abs/1907.11692},
	abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
	language = {en},
	urldate = {2022-03-13},
	journal = {arXiv:1907.11692 [cs]},
	author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
	month = jul,
	year = {2019},
	note = {arXiv: 1907.11692},
	keywords = {Computer Science - Computation and Language, READ},
}

@article{wang_glue_2019,
	title = {{GLUE}: {A} {Multi}-{Task} {Benchmark} and {Analysis} {Platform} for {Natural} {Language} {Understanding}},
	shorttitle = {{GLUE}},
	url = {http://arxiv.org/abs/1804.07461},
	abstract = {For natural language understanding (NLU) technology to be maximally useful, it must be able to process language in a way that is not exclusive to a single task, genre, or dataset. In pursuit of this objective, we introduce the General Language Understanding Evaluation (GLUE) benchmark, a collection of tools for evaluating the performance of models across a diverse set of existing NLU tasks. By including tasks with limited training data, GLUE is designed to favor and encourage models that share general linguistic knowledge across tasks. GLUE also includes a hand-crafted diagnostic test suite that enables detailed linguistic analysis of models. We evaluate baselines based on current methods for transfer and representation learning and ﬁnd that multi-task training on all tasks performs better than training a separate model per task. However, the low absolute performance of our best model indicates the need for improved general NLU systems.},
	language = {en},
	urldate = {2022-03-13},
	journal = {arXiv:1804.07461 [cs]},
	author = {Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
	month = feb,
	year = {2019},
	note = {arXiv: 1804.07461},
	keywords = {Computer Science - Computation and Language, READ},
}

@article{peters_deep_2018,
	title = {Deep contextualized word representations},
	url = {http://arxiv.org/abs/1802.05365},
	abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus. We show that these representations can be easily added to existing models and signiﬁcantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
	language = {en},
	urldate = {2022-03-06},
	journal = {arXiv:1802.05365 [cs]},
	author = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
	month = mar,
	year = {2018},
	note = {arXiv: 1802.05365},
	keywords = {Computer Science - Computation and Language, READ},
}

@article{lan_albert_2020,
	title = {{ALBERT}: {A} {Lite} {BERT} for {Self}-supervised {Learning} of {Language} {Representations}},
	shorttitle = {{ALBERT}},
	url = {http://arxiv.org/abs/1909.11942},
	abstract = {Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems, we present two parameterreduction techniques to lower memory consumption and increase the training speed of BERT (Devlin et al., 2019). Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and SQuAD benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at https://github.com/google-research/ALBERT.},
	language = {en},
	urldate = {2022-03-13},
	journal = {arXiv:1909.11942 [cs]},
	author = {Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
	month = feb,
	year = {2020},
	note = {arXiv: 1909.11942},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, READ},
}

@article{koban_self_2021,
	title = {The self in context: brain systems linking mental and physical health},
	volume = {22},
	issn = {1471-003X, 1471-0048},
	shorttitle = {The self in context},
	url = {http://www.nature.com/articles/s41583-021-00446-8},
	doi = {10.1038/s41583-021-00446-8},
	abstract = {Increasing evidence suggests that mental health and physical health are linked by neural systems that jointly regulate somatic physiology and high-level cognition. Key systems include the ventromedial prefrontal cortex and the related default-mode network. These systems help to construct models of the ‘self-in-c ontext’, compressing information across time and sensory modalities into conceptions of the underlying causes of experience. Self-in-c ontext models endow events with personal meaning and allow predictive control over behaviour and peripheral physiology, including autonomic, neuroendocrine and immune function. They guide learning from experience and the formation of narratives about the self and one’s world. Disorders of mental and physical health, especially those with high co-o ccurrence and convergent alterations in the functionality of the ventromedial prefrontal cortex and the default-mode network, could benefit from interventions focused on understanding and shaping mindsets and beliefs about the self, illness and treatment.},
	language = {en},
	number = {5},
	urldate = {2022-03-13},
	journal = {Nature Reviews Neuroscience},
	author = {Koban, Leonie and Gianaros, Peter J. and Kober, Hedy and Wager, Tor D.},
	month = may,
	year = {2021},
	keywords = {READ},
	pages = {309--322},
}

@article{noauthor_hmb_momentum_draft_nodate,
	title = {{HMB}\_Momentum\_Draft},
	keywords = {READ},
}

@misc{noauthor_notitle_nodate,
}

@article{hasson_hierarchical_2015,
	title = {Hierarchical process memory: memory as an integral component of information processing},
	volume = {19},
	issn = {13646613},
	shorttitle = {Hierarchical process memory},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1364661315000923},
	doi = {10.1016/j.tics.2015.04.006},
	language = {en},
	number = {6},
	urldate = {2022-03-13},
	journal = {Trends in Cognitive Sciences},
	author = {Hasson, Uri and Chen, Janice and Honey, Christopher J.},
	month = jun,
	year = {2015},
	keywords = {READ},
	pages = {304--313},
}

@article{lu_neural_2022,
	title = {A neural network model of when to retrieve and encode episodic memories},
	volume = {11},
	issn = {2050-084X},
	url = {https://doi.org/10.7554/eLife.74445},
	doi = {10.7554/eLife.74445},
	abstract = {Recent human behavioral and neuroimaging results suggest that people are selective in when they encode and retrieve episodic memories. To explain these findings, we trained a memory-augmented neural network to use its episodic memory to support prediction of upcoming states in an environment where past situations sometimes reoccur. We found that the network learned to retrieve selectively as a function of several factors, including its uncertainty about the upcoming state. Additionally, we found that selectively encoding episodic memories at the end of an event (but not mid-event) led to better subsequent prediction performance. In all of these cases, the benefits of selective retrieval and encoding can be explained in terms of reducing the risk of retrieving irrelevant memories. Overall, these modeling results provide a resource-rational account of why episodic retrieval and encoding should be selective and lead to several testable predictions.},
	urldate = {2022-03-13},
	journal = {eLife},
	author = {Lu, Qihong and Hasson, Uri and Norman, Kenneth A},
	editor = {Badre, David},
	month = feb,
	year = {2022},
	note = {Publisher: eLife Sciences Publications, Ltd},
	keywords = {READ},
	pages = {e74445},
}

@misc{noauthor_notitle_nodate-1,
}

@article{tay_transformer_2022,
	title = {Transformer {Memory} as a {Differentiable} {Search} {Index}},
	url = {http://arxiv.org/abs/2202.06991},
	abstract = {In this paper, we demonstrate that information retrieval can be accomplished with a single Transformer, in which all information about the corpus is encoded in the parameters of the model. To this end, we introduce the Diﬀerentiable Search Index (DSI), a new paradigm that learns a text-to-text model that maps string queries directly to relevant docids; in other words, a DSI model answers queries directly using only its parameters, dramatically simplifying the whole retrieval process. We study variations in how documents and their identiﬁers are represented, variations in training procedures, and the interplay between models and corpus sizes. Experiments demonstrate that given appropriate design choices, DSI signiﬁcantly outperforms strong baselines such as dual encoder models. Moreover, DSI demonstrates strong generalization capabilities, outperforming a BM25 baseline in a zero-shot setup.},
	language = {en},
	urldate = {2022-02-18},
	journal = {arXiv:2202.06991 [cs]},
	author = {Tay, Yi and Tran, Vinh Q. and Dehghani, Mostafa and Ni, Jianmo and Bahri, Dara and Mehta, Harsh and Qin, Zhen and Hui, Kai and Zhao, Zhe and Gupta, Jai and Schuster, Tal and Cohen, William W. and Metzler, Donald},
	month = feb,
	year = {2022},
	note = {arXiv: 2202.06991},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Information Retrieval, Computer Science - Machine Learning, READ},
}

@article{cowan_many_2017,
	title = {The many faces of working memory and short-term storage},
	volume = {24},
	issn = {1069-9384, 1531-5320},
	url = {http://link.springer.com/10.3758/s13423-016-1191-6},
	doi = {10.3758/s13423-016-1191-6},
	abstract = {The topic of working memory (WM) is ubiquitous in research on cognitive psychology and on individual differences. According to one definition, it is a small amount of information kept in a temporary state of heightened accessibility; it is used in most types of communication and problem solving. Short-term storage has been defined as the passive (i.e., non-attention-based, nonstrategic) component of WM or, alternatively, as a passive store separate from an attentionbased WM. Here I note that much confusion has been created by the use by various investigators of many, subtly different definitions of WM and short-term storage. The definitions are sometimes made explicit and sometimes implied. As I explain, the different definitions may have stemmed from the use of a wide variety of techniques to explore WM, along with differences in theoretical orientation. By delineating nine previously used definitions of WM and explaining how additional ones may emerge from combinations of these nine, I hope to improve scientific discourse on WM. The potential advantages of clarity about definitions of WM and short-term storage are illustrated with respect to several ongoing research controversies.},
	language = {en},
	number = {4},
	urldate = {2022-03-11},
	journal = {Psychonomic Bulletin \& Review},
	author = {Cowan, Nelson},
	month = aug,
	year = {2017},
	keywords = {READ},
	pages = {1158--1170},
}

@article{niven_probing_2019,
	title = {Probing {Neural} {Network} {Comprehension} of {Natural} {Language} {Arguments}},
	url = {http://arxiv.org/abs/1907.07355},
	abstract = {We are surprised to ﬁnd that BERT’s peak performance of 77\% on the Argument Reasoning Comprehension Task reaches just three points below the average untrained human baseline. However, we show that this result is entirely accounted for by exploitation of spurious statistical cues in the dataset. We analyze the nature of these cues and demonstrate that a range of models all exploit them. This analysis informs the construction of an adversarial dataset on which all models achieve random accuracy. Our adversarial dataset provides a more robust assessment of argument comprehension and should be adopted as the standard in future work.},
	language = {en},
	urldate = {2022-03-05},
	journal = {arXiv:1907.07355 [cs]},
	author = {Niven, Timothy and Kao, Hung-Yu},
	month = sep,
	year = {2019},
	note = {arXiv: 1907.07355},
	keywords = {Computer Science - Computation and Language, READ},
}

@article{groling_imitation_nodate,
	title = {Imitation {Learning} via {Reinforcement} {Learning} of {Fish} {Behaviour} with {Neural} {Networks}},
	abstract = {The collective behaviour of groups of animals emerges from interaction between individuals. Understanding these interindividual rules has always been a challenge, because the cognition of animals is not fully understood. Artiﬁcial neural networks in conjunction with attribution methods and others can help decipher these interindividual rules. In this thesis, an artiﬁcial neural network was trained with a recently proposed learning algorithm called Soft Q Imitation Learning (SQIL) on a dataset of two female guppies. The network is able to outperform a simple agent that uses the action of the most similar state in deﬁned metric and also is able to show most characteristics of ﬁsh, at least partially, when simulated.},
	author = {Gröling, Marc},
	keywords = {READ},
	pages = {29},
}

@article{oberauer_benchmarks_2018,
	title = {Benchmarks for models of short-term and working memory.},
	volume = {144},
	issn = {1939-1455, 0033-2909},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/bul0000153},
	doi = {10.1037/bul0000153},
	abstract = {Any mature field of research in psychology – such as short-term/working memory – is characterized by a wealth of empirical findings. It is currently unrealistic to expect a theory to explain them all; theorists must satisfice with explaining a subset of findings. The aim of the present article is to make the choice of that subset less arbitrary and idiosyncratic than is current practice. We propose criteria for identifying benchmark findings that every theory in a field should be able to explain: Benchmarks should be reproducible, generalize across materials and methodological variations, and be theoretically informative. We propose a set of benchmarks for theories and computational models of short-term and working memory. The benchmarks are described in as theory-neutral a way as possible, so that they can serve as empirical common ground for competing theoretical approaches. Benchmarks are rated on three levels according to their priority for explanation. Selection and ratings of the benchmarks is based on consensus among the authors, who jointly represent a broad range of theoretical perspectives on working memory, and they are supported by a survey among other experts on working memory. The article is accompanied by a web page providing an open forum for discussion; a site for submitting proposals for new benchmarks; and a repository for reference data sets for each benchmark.},
	language = {en},
	number = {9},
	urldate = {2022-02-26},
	journal = {Psychological Bulletin},
	author = {Oberauer, Klaus and Lewandowsky, Stephan and Awh, Edward and Brown, Gordon D. A. and Conway, Andrew and Cowan, Nelson and Donkin, Christopher and Farrell, Simon and Hitch, Graham J. and Hurlstone, Mark J. and Ma, Wei Ji and Morey, Candice C. and Nee, Derek Evan and Schweppe, Judith and Vergauwe, Evie and Ward, Geoff},
	month = sep,
	year = {2018},
	pages = {885--958},
}

@article{michael_asking_2020,
	title = {Asking without {Telling}: {Exploring} {Latent} {Ontologies} in {Contextual} {Representations}},
	shorttitle = {Asking without {Telling}},
	url = {http://arxiv.org/abs/2004.14513},
	abstract = {The success of pretrained contextual encoders, such as ELMo and BERT, has brought a great deal of interest in what these models learn: do they, without explicit supervision, learn to encode meaningful notions of linguistic structure? If so, how is this structure encoded? To investigate this, we introduce latent subclass learning (LSL): a modiﬁcation to classiﬁerbased probing that induces a latent categorization (or ontology) of the probe’s inputs. Without access to ﬁne-grained gold labels, LSL extracts emergent structure from input representations in an interpretable and quantiﬁable form. In experiments, we ﬁnd strong evidence of familiar categories, such as a notion of personhood in ELMo, as well as novel ontological distinctions, such as a preference for ﬁne-grained semantic roles on core arguments. Our results provide unique new evidence of emergent structure in pretrained encoders, including departures from existing annotations which are inaccessible to earlier methods.},
	language = {en},
	urldate = {2022-01-28},
	journal = {arXiv:2004.14513 [cs]},
	author = {Michael, Julian and Botha, Jan A. and Tenney, Ian},
	month = oct,
	year = {2020},
	note = {arXiv: 2004.14513},
	keywords = {Computer Science - Computation and Language, I.2.7, READ},
}

@article{rogers_primer_2020,
	title = {A {Primer} in {BERTology}: {What} {We} {Know} {About} {How} {BERT} {Works}},
	volume = {8},
	issn = {2307-387X},
	shorttitle = {A {Primer} in {BERTology}},
	url = {https://direct.mit.edu/tacl/article/96482},
	doi = {10.1162/tacl_a_00349},
	abstract = {Transformer-based models have pushed state of the art in many areas of NLP, but our understanding of what is behind their success is still limited. This paper is the first survey of over 150 studies of the popular BERT model. We review the current state of knowledge about how BERT works, what kind of information it learns and how it is represented, common modifications to its training objectives and architecture, the overparameterization issue, and approaches to compression. We then outline directions for future research.},
	language = {en},
	urldate = {2022-01-31},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Rogers, Anna and Kovaleva, Olga and Rumshisky, Anna},
	month = dec,
	year = {2020},
	keywords = {READ},
	pages = {842--866},
}

@article{merity_single_2019,
	title = {Single {Headed} {Attention} {RNN}: {Stop} {Thinking} {With} {Your} {Head}},
	shorttitle = {Single {Headed} {Attention} {RNN}},
	url = {http://arxiv.org/abs/1911.11423},
	abstract = {The leading approaches in language modeling are all obsessed with TV shows of my youth - namely Transformers and Sesame Street. Transformers this, Transformers that, and over here a bonﬁre worth of GPU-TPU-neuromorphic wafer scale silicon. We opt for the lazy path of old and proven techniques with a fancy crypto1 inspired acronym: the Single Headed Attention RNN (SHA-RNN). The author’s lone goal is to show that the entire ﬁeld might have evolved a different direction if we had instead been obsessed with a slightly different acronym and slightly different result. We take a previously strong language model based only on boring LSTMs and get it to within a stone’s throw of a stone’s throw of state-of-the-art byte level language model results on enwik8. This work has undergone no intensive hyperparameter optimization and lived entirely on a commodity desktop machine that made the author’s small studio apartment far too warm in the midst of a San Franciscan summer2. The ﬁnal results are achievable in plus or minus 24 hours on a single GPU as the author is impatient. The attention mechanism is also readily extended to large contexts with minimal computation. Take that Sesame Street.},
	language = {en},
	urldate = {2022-03-05},
	journal = {arXiv:1911.11423 [cs]},
	author = {Merity, Stephen},
	month = nov,
	year = {2019},
	note = {arXiv: 1911.11423},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing, READ},
}

@article{tenney_what_2019,
	title = {What do you learn from context? {Probing} for sentence structure in contextualized word representations},
	shorttitle = {What do you learn from context?},
	url = {http://arxiv.org/abs/1905.06316},
	abstract = {Contextualized representation models such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2018) have recently achieved state-of-the-art results on a diverse array of downstream NLP tasks. Building on recent token-level probing work, we introduce a novel edge probing task design and construct a broad suite of sub-sentence tasks derived from the traditional structured NLP pipeline. We probe word-level contextual representations from four recent models and investigate how they encode sentence structure across a range of syntactic, semantic, local, and long-range phenomena. We ﬁnd that existing models trained on language modeling and translation produce strong representations for syntactic phenomena, but only offer comparably small improvements on semantic tasks over a non-contextual baseline.},
	language = {en},
	urldate = {2022-02-27},
	journal = {arXiv:1905.06316 [cs]},
	author = {Tenney, Ian and Xia, Patrick and Chen, Berlin and Wang, Alex and Poliak, Adam and McCoy, R. Thomas and Kim, Najoung and Van Durme, Benjamin and Bowman, Samuel R. and Das, Dipanjan and Pavlick, Ellie},
	month = may,
	year = {2019},
	note = {arXiv: 1905.06316},
	keywords = {Computer Science - Computation and Language},
}

@article{razeghi_impact_2022,
	title = {Impact of {Pretraining} {Term} {Frequencies} on {Few}-{Shot} {Reasoning}},
	url = {http://arxiv.org/abs/2202.07206},
	abstract = {Pretrained Language Models (LMs) have demonstrated ability to perform numerical reasoning by extrapolating from a few examples in few-shot settings. However, the extent to which this extrapolation relies on robust reasoning is unclear. In this paper, we investigate how well these models reason with terms that are less frequent in the pretraining data. In particular, we examine the correlations between the model performance on test instances and the frequency of terms from those instances in the pretraining data. We measure the strength of this correlation for a number of GPT-based language models (pretrained on the Pile dataset) on various numerical deduction tasks (e.g., arithmetic and unit conversion). Our results consistently demonstrate that models are more accurate on instances whose terms are more prevalent, in some cases above 70\% (absolute) more accurate on the top 10\% frequent terms in comparison to the bottom 10\%. Overall, although LMs exhibit strong performance at few-shot numerical reasoning tasks, our results raise the question of how much models actually generalize beyond pretraining data, and we encourage researchers to take the pretraining data into account when interpreting evaluation results.},
	language = {en},
	urldate = {2022-03-11},
	journal = {arXiv:2202.07206 [cs]},
	author = {Razeghi, Yasaman and Logan IV, Robert L. and Gardner, Matt and Singh, Sameer},
	month = feb,
	year = {2022},
	note = {arXiv: 2202.07206},
	keywords = {Computer Science - Computation and Language, read},
}

@article{dyer_recurrent_2016,
	title = {Recurrent {Neural} {Network} {Grammars}},
	url = {http://arxiv.org/abs/1602.07776},
	abstract = {We introduce recurrent neural network grammars, probabilistic models of sentences with explicit phrase structure. We explain efﬁcient inference procedures that allow application to both parsing and language modeling. Experiments show that they provide better parsing in English than any single previously published supervised generative model and better language modeling than state-of-the-art sequential RNNs in English and Chinese1.},
	language = {en},
	urldate = {2022-03-09},
	journal = {arXiv:1602.07776 [cs]},
	author = {Dyer, Chris and Kuncoro, Adhiguna and Ballesteros, Miguel and Smith, Noah A.},
	month = oct,
	year = {2016},
	note = {arXiv: 1602.07776},
	keywords = {Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing},
}

@inproceedings{choe_parsing_2016,
	address = {Austin, Texas},
	title = {Parsing as {Language} {Modeling}},
	url = {http://aclweb.org/anthology/D16-1257},
	doi = {10.18653/v1/D16-1257},
	abstract = {We recast syntactic parsing as a language modeling problem and use recent advances in neural network language modeling to achieve a new state of the art for constituency Penn Treebank parsing — 93.8 F1 on section 23, using 2-21 as training, 24 as development, plus tri-training. When trees are converted to Stanford dependencies, UAS and LAS are 95.9\% and 94.1\%.},
	language = {en},
	urldate = {2022-03-09},
	booktitle = {Proceedings of the 2016 {Conference} on {Empirical} {Methods} in {Natural}           {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Choe, Do Kook and Charniak, Eugene},
	year = {2016},
	pages = {2331--2336},
}

@inproceedings{qian_structural_2021,
	address = {Online},
	title = {Structural {Guidance} for {Transformer} {Language} {Models}},
	url = {https://aclanthology.org/2021.acl-long.289},
	doi = {10.18653/v1/2021.acl-long.289},
	abstract = {Transformer-based language models pretrained on large amounts of text data have proven remarkably successful in learning generic transferable linguistic representations. Here we study whether structural guidance leads to more human-like systematic linguistic generalization in Transformer language models without resorting to pre-training on very large amounts of data. We explore two general ideas. The “Generative Parsing” idea jointly models the incremental parse and word sequence as part of the same sequence modeling task. The “Structural Scaffold” idea guides the language model’s representation via additional structure loss that separately predicts the incremental constituency parse. We train the proposed models along with a vanilla Transformer language model baseline on a 14 million-token and a 46 million-token subset of the BLLIP dataset, and evaluate models’ syntactic generalization performances on SG Test Suites and sized BLiMP. Experiment results across two benchmarks suggest converging evidence that generative structural supervisions can induce more robust and humanlike linguistic generalization in Transformer language models without the need for data intensive pre-training.},
	language = {en},
	urldate = {2022-03-09},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Qian, Peng and Naseem, Tahira and Levy, Roger and Fernandez Astudillo, Ramón},
	year = {2021},
	pages = {3735--3745},
}

@article{dai_transformer-xl_2019-1,
	title = {Transformer-{XL}: {Attentive} {Language} {Models} {Beyond} a {Fixed}-{Length} {Context}},
	shorttitle = {Transformer-{XL}},
	url = {http://arxiv.org/abs/1901.02860},
	abstract = {Transformers have a potential of learning longer-term dependency, but are limited by a ﬁxed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a ﬁxed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, TransformerXL learns dependency that is 80\% longer than RNNs and 450\% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-ofthe-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without ﬁnetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorﬂow and PyTorch1.},
	language = {en},
	urldate = {2022-03-09},
	journal = {arXiv:1901.02860 [cs, stat]},
	author = {Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc V. and Salakhutdinov, Ruslan},
	month = jun,
	year = {2019},
	note = {arXiv: 1901.02860},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning, read},
}

@article{sartran_transformer_2022,
	title = {Transformer {Grammars}: {Augmenting} {Transformer} {Language} {Models} with {Syntactic} {Inductive} {Biases} at {Scale}},
	shorttitle = {Transformer {Grammars}},
	url = {http://arxiv.org/abs/2203.00633},
	abstract = {Transformer language models that are trained on vast amounts of data have achieved remarkable success at various NLP benchmarks. Intriguingly, this success is achieved by models that lack an explicit modeling of hierarchical syntactic structures, which were hypothesized by decades of linguistic research to be necessary for good generalization. This naturally leaves a question: to what extent can we further improve the performance of Transformer language models, through an inductive bias that encourages the model to explain the data through the lens of recursive syntactic compositions? Although the beneﬁts of modeling recursive syntax have been shown at the small data and model scales, it remains an open question whether—and to what extent—a similar design principle is still beneﬁcial in the case of powerful Transformer language models that work well at scale. To answer these questions, we introduce Transformer Grammars—a novel class of Transformer language models that combine: (i) the expressive power, scalability, and strong performance of Transformers, and (ii) recursive syntactic compositions, which here are implemented through a special attention mask. We ﬁnd that Transformer Grammars outperform various strong baselines on multiple syntaxsensitive language modeling evaluation metrics, in addition to sentence-level language modeling perplexity. Nevertheless, we ﬁnd that the recursive syntactic composition bottleneck harms perplexity on document-level modeling, providing evidence that a different kind of memory mechanism—that works independently of syntactic structures—plays an important role in the processing of long-form text.},
	language = {en},
	urldate = {2022-03-09},
	journal = {arXiv:2203.00633 [cs]},
	author = {Sartran, Laurent and Barrett, Samuel and Kuncoro, Adhiguna and Stanojević, Miloš and Blunsom, Phil and Dyer, Chris},
	month = mar,
	year = {2022},
	note = {arXiv: 2203.00633},
	keywords = {Computer Science - Computation and Language},
}

@article{lei_object_2021,
	title = {Object {Based} {Attention} {Through} {Internal} {Gating}},
	url = {http://arxiv.org/abs/2106.04540},
	abstract = {Object-based attention is a key component of the visual system, relevant for perception, learning, and memory. Neurons tuned to features of attended objects tend to be more active than those associated with non-attended objects. There is a rich set of models of this phenomenon in computational neuroscience. However, there is currently a divide between models that successfully match physiological data but can only deal with extremely simple problems and models of attention used in computer vision. For example, attention in the brain is known to depend on top-down processing, whereas self-attention in deep learning does not. Here, we propose an artificial neural network model of object-based attention that captures the way in which attention is both top-down and recurrent. Our attention model works well both on simple test stimuli, such as those using images of handwritten digits, and on more complex stimuli, such as natural images drawn from the COCO dataset. We find that our model replicates a range of findings from neuroscience, including attention-invariant tuning, inhibition of return, and attention-mediated scaling of activity. Understanding object based attention is both computationally interesting and a key problem for computational neuroscience.},
	urldate = {2022-03-06},
	journal = {arXiv:2106.04540 [cs, q-bio]},
	author = {Lei, Jordan and Benjamin, Ari S. and Kording, Konrad P.},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.04540},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Quantitative Biology - Neurons and Cognition},
}

@article{oconnor_what_2021,
	title = {What {Context} {Features} {Can} {Transformer} {Language} {Models} {Use}?},
	url = {http://arxiv.org/abs/2106.08367},
	abstract = {Transformer-based language models benefit from conditioning on contexts of hundreds to thousands of previous tokens. What aspects of these contexts contribute to accurate model prediction? We describe a series of experiments that measure usable information by selectively ablating lexical and structural information in transformer language models trained on English Wikipedia. In both mid- and long-range contexts, we find that several extremely destructive context manipulations -- including shuffling word order within sentences and deleting all words other than nouns -- remove less than 15\% of the usable information. Our results suggest that long contexts, but not their detailed syntactic and propositional content, are important for the low perplexity of current transformer language models.},
	language = {en},
	urldate = {2022-01-04},
	journal = {arXiv:2106.08367 [cs]},
	author = {O'Connor, Joe and Andreas, Jacob},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.08367},
	keywords = {Computer Science - Computation and Language},
}

@article{rae_compressive_2019,
	title = {Compressive {Transformers} for {Long}-{Range} {Sequence} {Modelling}},
	url = {http://arxiv.org/abs/1911.05507},
	abstract = {We present the Compressive Transformer, an attentive sequence model which compresses past memories for long-range sequence learning. We ﬁnd the Compressive Transformer obtains state-of-the-art language modelling results in the WikiText-103 and Enwik8 benchmarks, achieving 17.1 ppl and 0.97 bpc respectively. We also ﬁnd it can model high-frequency speech effectively and can be used as a memory mechanism for RL, demonstrated on an object matching task. To promote the domain of long-range sequence learning, we propose a new openvocabulary language modelling benchmark derived from books, PG-19.},
	language = {en},
	urldate = {2022-03-05},
	journal = {arXiv:1911.05507 [cs, stat]},
	author = {Rae, Jack W. and Potapenko, Anna and Jayakumar, Siddhant M. and Lillicrap, Timothy P.},
	month = nov,
	year = {2019},
	note = {arXiv: 1911.05507},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{nematzadeh_memory_2020,
	title = {On {Memory} in {Human} and {Artifical} {Language} {Processing} {Systems}},
	url = {https://baicsworkshop.github.io/pdf/BAICS_22.pdf},
	abstract = {Memory in humans and artificial intelligence (AI) systems has similar functions— both are responsible for encoding, retrieving, and storing of information. While memory in humans has specialized systems for different functions (e.g., working memory, semantic memory, episodic memory), memory in AI systems is often implicitly represented in the weights of parametric neural networks. Focusing on language processing systems, we argue that this property makes it hard for AI systems to generalize across complex linguistic tasks. We consider the separation
of computation and storage as necessary, suggest desired properties of the storage system, and discuss the benefit of integrating different types of human memory (as separate modules) into next-generation language processing systems.},
	language = {en},
	urldate = {2022-03-05},
	author = {Nematzadeh, Aida and Ruder, Sebastian and Yogatama, Dani},
	year = {2020},
}

@article{cao_learning_2021,
	title = {Learning {Relation} {Prototype} from {Unlabeled} {Texts} for {Long}-tail {Relation} {Extraction}},
	issn = {1558-2191},
	doi = {10.1109/TKDE.2021.3096200},
	abstract = {Relation Extraction (RE) is a vital step to complete Knowledge Graph (KG) by extracting entity relations from texts. However, it usually suffers from the long-tail issue. This paper proposes a novel approach to learn relation prototypes from unlabeled texts, to facilitate long-tail RE by transferring knowledge from relation types with sufficient training data. We learn relation prototypes as an implicit factor between entities, which reflects meanings of relations and their proximities. We construct a co-occurrence graph from texts, and capture both first-order and second-order entity proximities for embedding learning. By optimize the distance from entity pairs to corresponding prototypes, our method can be easily adapted to almost arbitrary RE frameworks. Thus, the learning of infrequent or even unseen relation types will benefit from semantically proximate relations through pairs of entities and large-scale textual information. Extensive experiments on two publicly available datasets present promising improvements (4.1\% F1 on average). Ablation studies on long-tail relations, main components, and different RE models demonstrate the effectiveness of the learned relation prototypes. Finally, we analyze several example cases to give intuitive impressions as qualitative analysis. Our codes and data can be found in https://github.com/CrisJk/PA-TRP.},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Cao, Yixin and Kuang, Jun and Gao, Ming and Zhou, Aoying and Wen, Yonggang and Chua, Tat-Seng},
	year = {2021},
	note = {Conference Name: IEEE Transactions on Knowledge and Data Engineering},
	keywords = {Annotations, Data mining, Knowledge Graph, Prototype Learning, Prototypes, Relation Extraction, Training, Training data, Transfer learning, Urban areas, long-tail},
	pages = {1--1},
}

@article{paul_multi-facet_2021,
	title = {Multi-facet {Universal} {Schema}},
	url = {http://arxiv.org/abs/2103.15339},
	abstract = {Universal schema (USchema) assumes that two sentence patterns that share the same entity pairs are similar to each other. This assumption is widely adopted for solving various types of relation extraction (RE) tasks. Nevertheless, each sentence pattern could contain multiple facets, and not every facet is similar to all the facets of another sentence pattern cooccurring with the same entity pair. To address the violation of the USchema assumption, we propose multi-facet universal schema that uses a neural model to represent each sentence pattern as multiple facet embeddings and encourage one of these facet embeddings to be close to that of another sentence pattern if they cooccur with the same entity pair. In our experiments, we demonstrate that multi-facet embeddings signiﬁcantly outperform their singlefacet embedding counterpart, compositional universal schema (CUSchema) (Verga et al., 2016), in distantly supervised relation extraction tasks. Moreover, we can also use multiple embeddings to detect the entailment relation between two sentence patterns when no manual label is available.},
	language = {en},
	urldate = {2022-03-02},
	journal = {arXiv:2103.15339 [cs]},
	author = {Paul, Rohan and Chang, Haw-Shiuan and McCallum, Andrew},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.15339},
	keywords = {Computer Science - Computation and Language},
}

@article{mihindukulasooriya_leveraging_2020,
	title = {Leveraging {Semantic} {Parsing} for {Relation} {Linking} over {Knowledge} {Bases}},
	volume = {12506},
	url = {http://arxiv.org/abs/2009.07726},
	doi = {10.1007/978-3-030-62419-4_23},
	abstract = {Knowledge base question answering systems are heavily dependent on relation extraction and linking modules. However, the task of extracting and linking relations from text to knowledge bases faces two primary challenges; the ambiguity of natural language and lack of training data. To overcome these challenges, we present SLING, a relation linking framework which leverages semantic parsing using Abstract Meaning Representation (AMR) and distant supervision. SLING integrates multiple approaches that capture complementary signals such as linguistic cues, rich semantic representation, and information from the knowledge base. The experiments on relation linking using three KBQA datasets, QALD-7, QALD-9, and LC-QuAD 1.0 demonstrate that the proposed approach achieves state-of-the-art performance on all benchmarks.},
	language = {en},
	urldate = {2022-03-02},
	journal = {arXiv:2009.07726 [cs]},
	author = {Mihindukulasooriya, Nandana and Rossiello, Gaetano and Kapanipathi, Pavan and Abdelaziz, Ibrahim and Ravishankar, Srinivas and Yu, Mo and Gliozzo, Alfio and Roukos, Salim and Gray, Alexander},
	year = {2020},
	note = {arXiv: 2009.07726},
	keywords = {68T35, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, I.2.4, I.2.7},
	pages = {402--419},
}

@inproceedings{chen_relation_2021,
	address = {Online},
	title = {Relation {Extraction} with {Type}-aware {Map} {Memories} of {Word} {Dependencies}},
	url = {https://aclanthology.org/2021.findings-acl.221},
	doi = {10.18653/v1/2021.findings-acl.221},
	language = {en},
	urldate = {2022-03-02},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL}-{IJCNLP} 2021},
	publisher = {Association for Computational Linguistics},
	author = {Chen, Guimin and Tian, Yuanhe and Song, Yan and Wan, Xiang},
	year = {2021},
	pages = {2501--2512},
}

@article{xu_syntax-enhanced_2021,
	title = {Syntax-{Enhanced} {Pre}-trained {Model}},
	url = {http://arxiv.org/abs/2012.14116},
	abstract = {We study the problem of leveraging the syntactic structure of text to enhance pre-trained models such as BERT and RoBERTa. Existing methods utilize syntax of text either in the pre-training stage or in the fine-tuning stage, so that they suffer from discrepancy between the two stages. Such a problem would lead to the necessity of having human-annotated syntactic information, which limits the application of existing methods to broader scenarios. To address this, we present a model that utilizes the syntax of text in both pre-training and fine-tuning stages. Our model is based on Transformer with a syntax-aware attention layer that considers the dependency tree of the text. We further introduce a new pre-training task of predicting the syntactic distance among tokens in the dependency tree. We evaluate the model on three downstream tasks, including relation classification, entity typing, and question answering. Results show that our model achieves state-of-the-art performance on six public benchmark datasets. We have two major findings. First, we demonstrate that infusing automatically produced syntax of text improves pre-trained models. Second, global syntactic distances among tokens bring larger performance gains compared to local head relations between contiguous tokens.},
	language = {en},
	urldate = {2022-03-02},
	journal = {arXiv:2012.14116 [cs]},
	author = {Xu, Zenan and Guo, Daya and Tang, Duyu and Su, Qinliang and Shou, Linjun and Gong, Ming and Zhong, Wanjun and Quan, Xiaojun and Duan, Nan and Jiang, Daxin},
	month = may,
	year = {2021},
	note = {arXiv: 2012.14116},
	keywords = {Computer Science - Computation and Language},
}

@article{qin_erica_2021,
	title = {{ERICA}: {Improving} {Entity} and {Relation} {Understanding} for {Pre}-trained {Language} {Models} via {Contrastive} {Learning}},
	shorttitle = {{ERICA}},
	url = {http://arxiv.org/abs/2012.15022},
	abstract = {Pre-trained Language Models (PLMs) have shown superior performance on various downstream Natural Language Processing (NLP) tasks. However, conventional pre-training objectives do not explicitly model relational facts in text, which are crucial for textual understanding. To address this issue, we propose a novel contrastive learning framework ERICA to obtain a deep understanding of the entities and their relations in text. Specifically, we define two novel pre-training tasks to better understand entities and relations: (1) the entity discrimination task to distinguish which tail entity can be inferred by the given head entity and relation; (2) the relation discrimination task to distinguish whether two relations are close or not semantically, which involves complex relational reasoning. Experimental results demonstrate that ERICA can improve typical PLMs (BERT and RoBERTa) on several language understanding tasks, including relation extraction, entity typing and question answering, especially under low-resource settings.},
	language = {en},
	urldate = {2022-03-02},
	journal = {arXiv:2012.15022 [cs]},
	author = {Qin, Yujia and Lin, Yankai and Takanobu, Ryuichi and Liu, Zhiyuan and Li, Peng and Ji, Heng and Huang, Minlie and Sun, Maosong and Zhou, Jie},
	month = may,
	year = {2021},
	note = {arXiv: 2012.15022},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@article{heinzerling_language_2021,
	title = {Language {Models} as {Knowledge} {Bases}: {On} {Entity} {Representations}, {Storage} {Capacity}, and {Paraphrased} {Queries}},
	shorttitle = {Language {Models} as {Knowledge} {Bases}},
	url = {http://arxiv.org/abs/2008.09036},
	abstract = {Pretrained language models have been suggested as a possible alternative or complement to structured knowledge bases. However, this emerging LM-as-KB paradigm has so far only been considered in a very limited setting, which only allows handling 21k entities whose name is found in common LM vocabularies. Furthermore, a major beneﬁt of this paradigm, i.e., querying the KB using natural language paraphrases, is underexplored. Here we formulate two basic requirements for treating LMs as KBs: (i) the ability to store a large number facts involving a large number of entities and (ii) the ability to query stored facts. We explore three entity representations that allow LMs to handle millions of entities and present a detailed case study on paraphrased querying of facts stored in LMs, thereby providing a proof-of-concept that language models can indeed serve as knowledge bases.},
	language = {en},
	urldate = {2022-03-02},
	journal = {arXiv:2008.09036 [cs]},
	author = {Heinzerling, Benjamin and Inui, Kentaro},
	month = apr,
	year = {2021},
	note = {arXiv: 2008.09036},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{tong_improving_2020,
	address = {Online},
	title = {Improving {Event} {Detection} via {Open}-domain {Trigger} {Knowledge}},
	url = {https://aclanthology.org/2020.acl-main.522},
	doi = {10.18653/v1/2020.acl-main.522},
	abstract = {Event Detection (ED) is a fundamental task in automatically structuring texts. Due to the small scale of training data, previous methods perform poorly on unseen/sparsely labeled trigger words and are prone to overfitting densely labeled trigger words. To address the issue, we propose a novel Enrichment Knowledge Distillation (EKD) model to leverage external open-domain trigger knowledge to reduce the in-built biases to frequent trigger words in annotations. Experiments on benchmark ACE2005 show that our model outperforms nine strong baselines, is especially effective for unseen/sparsely labeled trigger words. The source code is released on https://github.com/shuaiwa16/ekd.git.},
	urldate = {2022-03-02},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Tong, Meihan and Xu, Bin and Wang, Shuai and Cao, Yixin and Hou, Lei and Li, Juanzi and Xie, Jun},
	month = jul,
	year = {2020},
	pages = {5887--5897},
}

@article{fevry_entities_2020,
	title = {Entities as {Experts}: {Sparse} {Memory} {Access} with {Entity} {Supervision}},
	shorttitle = {Entities as {Experts}},
	url = {http://arxiv.org/abs/2004.07202},
	abstract = {We focus on the problem of capturing declarative knowledge about entities in the learned parameters of a language model. We introduce a new model - Entities as Experts (EAE) - that can access distinct memories of the entities mentioned in a piece of text. Unlike previous efforts to integrate entity knowledge into sequence models, EAE's entity representations are learned directly from text. We show that EAE's learned representations capture sufficient knowledge to answer TriviaQA questions such as "Which Dr. Who villain has been played by Roger Delgado, Anthony Ainley, Eric Roberts?", outperforming an encoder-generator Transformer model with 10x the parameters. According to the LAMA knowledge probes, EAE contains more factual knowledge than a similarly sized BERT, as well as previous approaches that integrate external sources of entity knowledge. Because EAE associates parameters with specific entities, it only needs to access a fraction of its parameters at inference time, and we show that the correct identification and representation of entities is essential to EAE's performance.},
	urldate = {2022-03-02},
	journal = {arXiv:2004.07202 [cs]},
	author = {Févry, Thibault and Soares, Livio Baldini and FitzGerald, Nicholas and Choi, Eunsol and Kwiatkowski, Tom},
	month = oct,
	year = {2020},
	note = {arXiv: 2004.07202},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{han_opennre_2019,
	title = {{OpenNRE}: {An} {Open} and {Extensible} {Toolkit} for {Neural} {Relation} {Extraction}},
	shorttitle = {{OpenNRE}},
	url = {http://arxiv.org/abs/1909.13078},
	abstract = {OpenNRE is an open-source and extensible toolkit that provides a uniﬁed framework to implement neural models for relation extraction (RE). Speciﬁcally, by implementing typical RE methods, OpenNRE not only allows developers to train custom models to extract structured relational facts from the plain text but also supports quick model validation for researchers. Besides, OpenNRE provides various functional RE modules based on both TensorFlow and PyTorch to maintain sufﬁcient modularity and extensibility, making it becomes easy to incorporate new models into the framework. Besides the toolkit, we also release an online system to meet real-time extraction without any training and deploying. Meanwhile, the online system can extract facts in various scenarios as well as aligning the extracted facts to Wikidata, which may beneﬁt various downstream knowledge-driven applications (e.g., information retrieval and question answering). More details of the toolkit and online system can be obtained from http://github.com/ thunlp/OpenNRE.},
	language = {en},
	urldate = {2022-03-02},
	journal = {arXiv:1909.13078 [cs]},
	author = {Han, Xu and Gao, Tianyu and Yao, Yuan and Ye, Demin and Liu, Zhiyuan and Sun, Maosong},
	month = sep,
	year = {2019},
	note = {arXiv: 1909.13078},
	keywords = {Computer Science - Computation and Language},
}

@article{gao_fewrel_2019,
	title = {{FewRel} 2.0: {Towards} {More} {Challenging} {Few}-{Shot} {Relation} {Classification}},
	shorttitle = {{FewRel} 2.0},
	url = {http://arxiv.org/abs/1910.07124},
	abstract = {We present FewRel 2.0, a more challenging task to investigate two aspects of few-shot relation classiﬁcation models: (1) Can they adapt to a new domain with only a handful of instances? (2) Can they detect noneof-the-above (NOTA) relations? To construct FewRel 2.0, we build upon the FewRel dataset (Han et al., 2018) by adding a new test set in a quite different domain, and a NOTA relation choice. With the new dataset and extensive experimental analysis, we found (1) that the state-of-the-art few-shot relation classiﬁcation models struggle on these two aspects, and (2) that the commonly-used techniques for domain adaptation and NOTA detection still cannot handle the two challenges well. Our research calls for more attention and further efforts to these two real-world issues. All details and resources about the dataset and baselines are released at https: //github.com/thunlp/fewrel.},
	language = {en},
	urldate = {2022-03-02},
	journal = {arXiv:1910.07124 [cs]},
	author = {Gao, Tianyu and Han, Xu and Zhu, Hao and Liu, Zhiyuan and Li, Peng and Sun, Maosong and Zhou, Jie},
	month = oct,
	year = {2019},
	note = {arXiv: 1910.07124},
	keywords = {Computer Science - Computation and Language},
}

@article{wang_kepler_2021,
	title = {{KEPLER}: {A} {Unified} {Model} for {Knowledge} {Embedding} and {Pre}-trained {Language} {Representation}},
	volume = {9},
	issn = {2307-387X},
	shorttitle = {{KEPLER}},
	url = {https://doi.org/10.1162/tacl_a_00360},
	doi = {10.1162/tacl_a_00360},
	abstract = {Pre-trained language representation models (PLMs) cannot well capture factual knowledge from text. In contrast, knowledge embedding (KE) methods can effectively represent the relational facts in knowledge graphs (KGs) with informative entity embeddings, but conventional KE models cannot take full advantage of the abundant textual information. In this paper, we propose a unified model for Knowledge Embedding and Pre-trained LanguagERepresentation (KEPLER), which can not only better integrate factual knowledge into PLMs but also produce effective text-enhanced KE with the strong PLMs. In KEPLER, we encode textual entity descriptions with a PLM as their embeddings, and then jointly optimize the KE and language modeling objectives. Experimental results show that KEPLER achieves state-of-the-art performances on various NLP tasks, and also works remarkably well as an inductive KE model on KG link prediction. Furthermore, for pre-training and evaluating KEPLER, we construct Wikidata5M1 , a large-scale KG dataset with aligned entity descriptions, and benchmark state-of-the-art KE methods on it. It shall serve as a new KE benchmark and facilitate the research on large KG, inductive KE, and KG with text. The source code can be obtained from https://github.com/THU-KEG/KEPLER.},
	urldate = {2022-03-02},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Wang, Xiaozhi and Gao, Tianyu and Zhu, Zhaocheng and Zhang, Zhengyan and Liu, Zhiyuan and Li, Juanzi and Tang, Jian},
	month = mar,
	year = {2021},
	pages = {176--194},
}

@article{peters_knowledge_2019,
	title = {Knowledge {Enhanced} {Contextual} {Word} {Representations}},
	url = {http://arxiv.org/abs/1909.04164},
	abstract = {Contextual word representations, typically trained on unstructured, unlabeled text, do not contain any explicit grounding to real world entities and are often unable to remember facts about those entities. We propose a general method to embed multiple knowledge bases (KBs) into large scale models, and thereby enhance their representations with structured, human-curated knowledge. For each KB, we ﬁrst use an integrated entity linker to retrieve relevant entity embeddings, then update contextual word representations via a form of word-to-entity attention. In contrast to previous approaches, the entity linkers and selfsupervised language modeling objective are jointly trained end-to-end in a multitask setting that combines a small amount of entity linking supervision with a large amount of raw text. After integrating WordNet and a subset of Wikipedia into BERT, the knowledge enhanced BERT (KnowBert) demonstrates improved perplexity, ability to recall facts as measured in a probing task and downstream performance on relationship extraction, entity typing, and word sense disambiguation. KnowBert’s runtime is comparable to BERT’s and it scales to large KBs.},
	language = {en},
	urldate = {2022-03-02},
	journal = {arXiv:1909.04164 [cs]},
	author = {Peters, Matthew E. and Neumann, Mark and Logan IV, Robert L. and Schwartz, Roy and Joshi, Vidur and Singh, Sameer and Smith, Noah A.},
	month = oct,
	year = {2019},
	note = {arXiv: 1909.04164},
	keywords = {Computer Science - Computation and Language},
}

@article{kovaleva_revealing_2019,
	title = {Revealing the {Dark} {Secrets} of {BERT}},
	url = {http://arxiv.org/abs/1908.08593},
	abstract = {BERT-based architectures currently give stateof-the-art performance on many NLP tasks, but little is known about the exact mechanisms that contribute to its success. In the current work, we focus on the interpretation of selfattention, which is one of the fundamental underlying components of BERT. Using a subset of GLUE tasks and a set of handcrafted features-of-interest, we propose the methodology and carry out a qualitative and quantitative analysis of the information encoded by the individual BERT’s heads. Our ﬁndings suggest that there is a limited set of attention patterns that are repeated across different heads, indicating the overall model overparametrization. While different heads consistently use the same attention patterns, they have varying impact on performance across different tasks. We show that manually disabling attention in certain heads leads to a performance improvement over the regular ﬁne-tuned BERT models.},
	language = {en},
	urldate = {2022-01-27},
	journal = {arXiv:1908.08593 [cs, stat]},
	author = {Kovaleva, Olga and Romanov, Alexey and Rogers, Anna and Rumshisky, Anna},
	month = sep,
	year = {2019},
	note = {arXiv: 1908.08593},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, READ, Statistics - Machine Learning},
}

@article{mccoy_right_2019,
	title = {Right for the {Wrong} {Reasons}: {Diagnosing} {Syntactic} {Heuristics} in {Natural} {Language} {Inference}},
	shorttitle = {Right for the {Wrong} {Reasons}},
	url = {http://arxiv.org/abs/1902.01007},
	abstract = {A machine learning system can score well on a given test set by relying on heuristics that are effective for frequent example types but break down in more challenging cases. We study this issue within natural language inference (NLI), the task of determining whether one sentence entails another. We hypothesize that statistical NLI models may adopt three fallible syntactic heuristics: the lexical overlap heuristic, the subsequence heuristic, and the constituent heuristic. To determine whether models have adopted these heuristics, we introduce a controlled evaluation set called HANS (Heuristic Analysis for NLI Systems), which contains many examples where the heuristics fail. We ﬁnd that models trained on MNLI, including BERT, a state-of-the-art model, perform very poorly on HANS, suggesting that they have indeed adopted these heuristics. We conclude that there is substantial room for improvement in NLI systems, and that the HANS dataset can motivate and measure progress in this area.},
	language = {en},
	urldate = {2022-02-27},
	journal = {arXiv:1902.01007 [cs]},
	author = {McCoy, R. Thomas and Pavlick, Ellie and Linzen, Tal},
	month = jun,
	year = {2019},
	note = {arXiv: 1902.01007},
	keywords = {Computer Science - Computation and Language, READ},
}

@article{ettinger_what_2020,
	title = {What {BERT} {Is} {Not}: {Lessons} from a {New} {Suite} of {Psycholinguistic} {Diagnostics} for {Language} {Models}},
	volume = {8},
	issn = {2307-387X},
	shorttitle = {What {BERT} {Is} {Not}},
	url = {https://doi.org/10.1162/tacl_a_00298},
	doi = {10.1162/tacl_a_00298},
	abstract = {Pre-training by language modeling has become a popular and successful approach to NLP tasks, but we have yet to understand exactly what linguistic capacities these pre-training processes confer upon models. In this paper we introduce a suite of diagnostics drawn from human language experiments, which allow us to ask targeted questions about information used by language models for generating predictions in context. As a case study, we apply these diagnostics to the popular BERT model, finding that it can generally distinguish good from bad completions involving shared category or role reversal, albeit with less sensitivity than humans, and it robustly retrieves noun hypernyms, but it struggles with challenging inference and role-based event prediction— and, in particular, it shows clear insensitivity to the contextual impacts of negation.},
	urldate = {2022-02-18},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Ettinger, Allyson},
	month = jan,
	year = {2020},
	keywords = {READ},
	pages = {34--48},
}

@article{bisk_experience_2020,
	title = {Experience {Grounds} {Language}},
	url = {http://arxiv.org/abs/2004.10151},
	abstract = {Language understanding research is held back by a failure to relate language to the physical world it describes and to the social interactions it facilitates. Despite the incredible effectiveness of language processing models to tackle tasks after being trained on text alone, successful linguistic communication relies on a shared experience of the world. It is this shared experience that makes utterances meaningful.},
	language = {en},
	urldate = {2022-02-27},
	journal = {arXiv:2004.10151 [cs]},
	author = {Bisk, Yonatan and Holtzman, Ari and Thomason, Jesse and Andreas, Jacob and Bengio, Yoshua and Chai, Joyce and Lapata, Mirella and Lazaridou, Angeliki and May, Jonathan and Nisnevich, Aleksandr and Pinto, Nicolas and Turian, Joseph},
	month = nov,
	year = {2020},
	note = {arXiv: 2004.10151},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, READ},
}

@article{howard_distributed_2015,
	title = {A distributed representation of internal time.},
	volume = {122},
	issn = {1939-1471, 0033-295X},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/a0037840},
	doi = {10.1037/a0037840},
	abstract = {This article pursues the hypothesis that a scale-invariant representation of history could support performance in a variety of learning and memory tasks. This representation maintains a conjunctive representation of what happened when that grows continuously less accurate for events further and further in the past. Simple behavioral models using a few operations, including scanning, matching and a “jump back in time” that recovers previous states of the history, describe a range of behavioral phenomena. These behavioral applications include canonical results from the judgment of recency task over short and long scales, the recency and contiguity effect across scales in episodic recall, and temporal mapping phenomena in conditioning. A growing body of neural data suggests that neural representations in several brain regions have qualitative properties predicted by the representation of temporal history. Taken together, these results suggest that a scale-invariant representation of temporal history may serve as a cornerstone of a physical model of cognition in learning and memory.},
	language = {en},
	number = {1},
	urldate = {2022-02-23},
	journal = {Psychological Review},
	author = {Howard, Marc W. and Shankar, Karthik H. and Aue, William R. and Criss, Amy H.},
	month = jan,
	year = {2015},
	pages = {24--53},
}

@article{johnson_fast_nodate,
	title = {Fast and ﬂexible: {Human} program induction in abstract reasoning tasks},
	abstract = {The Abstraction and Reasoning Corpus (ARC) is a collection program induction tasks that was recently proposed by Chollet (2019) as a measure of machine intelligence. Here, we report a preliminary set of results from a behavioral study of humans solving a subset of tasks from ARC (40 out of 1000). We found that humans were able to infer the underlying program and generate the correct test output for a novel test input example, with an average of 84\% of tasks solved per participant, and with 65\% of tasks being solved by more than 80\% of participants. Additionally, we ﬁnd interesting patterns of behavioral consistency and variability across the action sequences to generate their responses, the natural language descriptions used to describe the rule for each task, and the errors people make. Our ﬁndings suggest that people can quickly and reliably determine the relevant features and properties of a task to compose a correct solution, despite limited experience in this domain. This dataset offers useful insights for designing AI systems that can solve abstract reasoning tasks such as ARC with the ﬂuidity of human intelligence.},
	language = {en},
	author = {Johnson, Aysja and Lake, Brenden M},
	pages = {7},
}

@article{lake_human-level_2015,
	title = {Human-level concept learning through probabilistic program induction},
	volume = {350},
	issn = {0036-8075, 1095-9203},
	url = {https://www.sciencemag.org/lookup/doi/10.1126/science.aab3050},
	doi = {10.1126/science.aab3050},
	language = {en},
	number = {6266},
	urldate = {2022-02-10},
	journal = {Science},
	author = {Lake, B. M. and Salakhutdinov, R. and Tenenbaum, J. B.},
	month = dec,
	year = {2015},
	pages = {1332--1338},
}

@article{lake_building_2016,
	title = {Building {Machines} {That} {Learn} and {Think} {Like} {People}},
	url = {http://arxiv.org/abs/1604.00289},
	abstract = {Recent progress in artiﬁcial intelligence (AI) has renewed interest in building systems that learn and think like people. Many advances have come from using deep neural networks trained end-to-end in tasks such as object recognition, video games, and board games, achieving performance that equals or even beats humans in some respects. Despite their biological inspiration and performance achievements, these systems diﬀer from human intelligence in crucial ways. We review progress in cognitive science suggesting that truly human-like learning and thinking machines will have to reach beyond current engineering trends in both what they learn, and how they learn it. Speciﬁcally, we argue that these machines should (a) build causal models of the world that support explanation and understanding, rather than merely solving pattern recognition problems; (b) ground learning in intuitive theories of physics and psychology, to support and enrich the knowledge that is learned; and (c) harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations. We suggest concrete challenges and promising routes towards these goals that can combine the strengths of recent neural network advances with more structured cognitive models.},
	language = {en},
	urldate = {2022-02-10},
	journal = {arXiv:1604.00289 [cs, stat]},
	author = {Lake, Brenden M. and Ullman, Tomer D. and Tenenbaum, Joshua B. and Gershman, Samuel J.},
	month = nov,
	year = {2016},
	note = {arXiv: 1604.00289},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{linzen_syntactic_2021,
	title = {Syntactic {Structure} from {Deep} {Learning}},
	volume = {7},
	issn = {2333-9683, 2333-9691},
	url = {http://arxiv.org/abs/2004.10827},
	doi = {10.1146/annurev-linguistics-032020-051035},
	abstract = {Modern deep neural networks achieve impressive performance in engineering applications that require extensive linguistic skills, such as machine translation. This success has sparked interest in probing whether these models are inducing human-like grammatical knowledge from the raw data they are exposed to, and, consequently, whether they can shed new light on long-standing debates concerning the innate structure necessary for language acquisition. In this article, we survey representative studies of the syntactic abilities of deep networks, and discuss the broader implications that this work has for theoretical linguistics.},
	language = {en},
	number = {1},
	urldate = {2022-02-09},
	journal = {Annual Review of Linguistics},
	author = {Linzen, Tal and Baroni, Marco},
	month = jan,
	year = {2021},
	note = {arXiv: 2004.10827},
	keywords = {Computer Science - Computation and Language},
	pages = {195--212},
}

@article{lake_human_2019,
	title = {Human few-shot learning of compositional instructions},
	url = {http://arxiv.org/abs/1901.04587},
	abstract = {People learn in fast and ﬂexible ways that have not been emulated by machines. Once a person learns a new verb “dax,” he or she can effortlessly understand how to “dax twice,” “walk and dax,” or “dax vigorously.” There have been striking recent improvements in machine learning for natural language processing, yet the best algorithms require vast amounts of experience and struggle to generalize new concepts in compositional ways. To better understand these distinctively human abilities, we study the compositional skills of people through languagelike instruction learning tasks. Our results show that people can learn and use novel functional concepts from very few examples (few-shot learning), successfully applying familiar functions to novel inputs. People can also compose concepts in complex ways that go beyond the provided demonstrations. Two additional experiments examined the assumptions and inductive biases that people make when solving these tasks, revealing three biases: mutual exclusivity, one-to-one mappings, and iconic concatenation. We discuss the implications for cognitive modeling and the potential for building machines with more human-like language learning capabilities.},
	language = {en},
	urldate = {2022-02-09},
	journal = {arXiv:1901.04587 [cs]},
	author = {Lake, Brenden M. and Linzen, Tal and Baroni, Marco},
	month = may,
	year = {2019},
	note = {arXiv: 1901.04587},
	keywords = {Computer Science - Computation and Language},
}

@article{gulordava_colorless_2018,
	title = {Colorless green recurrent networks dream hierarchically},
	url = {http://arxiv.org/abs/1803.11138},
	abstract = {Recurrent neural networks (RNNs) have achieved impressive results in a variety of linguistic processing tasks, suggesting that they can induce non-trivial properties of language. We investigate here to what extent RNNs learn to track abstract hierarchical syntactic structure. We test whether RNNs trained with a generic language modeling objective in four languages (Italian, English, Hebrew, Russian) can predict long-distance number agreement in various constructions. We include in our evaluation nonsensical sentences where RNNs cannot rely on semantic or lexical cues (“The colorless green iiddeeaass I ate with the chair sslleeeepp furiously”), and, for Italian, we compare model performance to human intuitions. Our language-model-trained RNNs make reliable predictions about long-distance agreement, and do not lag much behind human performance. We thus bring support to the hypothesis that RNNs are not just shallowpattern extractors, but they also acquire deeper grammatical competence.},
	language = {en},
	urldate = {2022-02-09},
	journal = {arXiv:1803.11138 [cs]},
	author = {Gulordava, Kristina and Bojanowski, Piotr and Grave, Edouard and Linzen, Tal and Baroni, Marco},
	month = mar,
	year = {2018},
	note = {arXiv: 1803.11138},
	keywords = {Computer Science - Computation and Language},
}

@article{linzen_how_2020,
	title = {How {Can} {We} {Accelerate} {Progress} {Towards} {Human}-like {Linguistic} {Generalization}?},
	url = {http://arxiv.org/abs/2005.00955},
	abstract = {This position paper describes and critiques the Pretraining-Agnostic Identically Distributed (PAID) evaluation paradigm, which has become a central tool for measuring progress in natural language understanding. This paradigm consists of three stages: (1) pretraining of a word prediction model on a corpus of arbitrary size; (2) ﬁne-tuning (transfer learning) on a training set representing a classiﬁcation task; (3) evaluation on a test set drawn from the same distribution as that training set. This paradigm favors simple, low-bias architectures, which, ﬁrst, can be scaled to process vast amounts of data, and second, can capture the ﬁne-grained statistical properties of a particular data set, regardless of whether those properties are likely to generalize to examples of the task outside the data set. This contrasts with humans, who learn language from several orders of magnitude less data than the systems favored by this evaluation paradigm, and generalize to new tasks in a consistent way. We advocate for supplementing or replacing PAID with paradigms that reward architectures that generalize as quickly and robustly as humans.},
	language = {en},
	urldate = {2022-02-09},
	journal = {arXiv:2005.00955 [cs]},
	author = {Linzen, Tal},
	month = may,
	year = {2020},
	note = {arXiv: 2005.00955},
	keywords = {Computer Science - Computation and Language},
}

@article{van_rullen_face_1998,
	title = {Face processing using one spike per neurone},
	volume = {48},
	issn = {03032647},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0303264798000707},
	doi = {10.1016/S0303-2647(98)00070-7},
	abstract = {The speed with which neurones in the monkey temporal lobe can respond selectively to the presence of a face implies that processing may be possible using only one spike per neurone, a ﬁnding that is problematic for conventional rate coding models that need at least two spikes to estimate interspike interval. One way of avoiding this problem uses the fact that integrate-and-ﬁre neurones will tend to ﬁre at different times, with the most strongly activated neurones ﬁring ﬁrst (Thorpe, 1990, Parallel Processing in Neural Systems). Under such conditions, processing can be performed by using the order in which cells in a particular layer ﬁre as a code. To test this idea, we have explored a range of architectures using SpikeNET (Thorpe and Gautrais, 1997, Neural Information Processing Systems, 9), a simulator designed for modelling large populations of integrate-and-ﬁre neurones. One such network used a simple four-layer feed-forward architecture to detect and localise the presence of human faces in natural images. Performance of the model was tested with a large range of grey-scale images of faces and other objects and was found to be remarkably good by comparison with more classic image processing techniques. The most remarkable feature of these results is that they were obtained using a purely feed-forward neural network in which none of the neurones ﬁred more than one spike (thus ruling out conventional rate coding mechanisms). It thus appears that the combination of asynchronous spike propagation and rank order coding may provide an important key to understanding how the nervous system can achieve such a huge amount of processing in so little time. © 1998 Elsevier Science Ireland Ltd. All rights reserved.},
	language = {en},
	number = {1-3},
	urldate = {2022-02-07},
	journal = {Biosystems},
	author = {Van Rullen, Rufin and Gautrais, Jacques and Delorme, Arnaud and Thorpe, Simon},
	month = nov,
	year = {1998},
	pages = {229--239},
}

@article{thorpe_ultra_nodate,
	title = {{ULTRA} {RAPID} {FACE} {DETECTION} {IN} {NATURAL} {IMAGES} : {IMPLICATIONS} {FOR} {COMPUTATION} {IN} {THE} {VISUAL} {SYSTEM}},
	abstract = {Using a choice saccade task, Kirchner and Thorpe recently demonstrated that detection of animals in natural scenes is considerably faster than previously supposed [1]. Here we present some new data with the same task that show that face detection is even more efficient. When two images are flashed to the left and right of fixation, subjects can accurately make saccades to the side where there is a human face with a mean reaction of time of only 147 ms and an accuracy level of 94\%. The earliest reliable saccades were made as early as 110 ms after stimulus onset. If we allow roughly 20 ms for saccade initiation, such data leaves very little time for visual processing and seems to rule out computations that require more than one spike per neuron. Furthermore, it seems clear that only a feedforward pass through the visual pathways can be performed in so little time.},
	language = {en},
	author = {Thorpe, Simon J and Crouzet, Sébastien and Kirchner, Holle and Fabre-Thorpe, Michèle and Sabatier, Université Paul},
	pages = {5},
}

@techreport{rahnev_is_2021,
	type = {preprint},
	title = {Is perception probabilistic? {Clarifying} the definitions},
	shorttitle = {Is perception probabilistic?},
	url = {https://osf.io/f8v5r},
	abstract = {A fundamental question for perception research is what sensory information is available for decision making, or, stated differently, what is the output of perception. One answer that has emerged in the last two decades is that perception is probabilistic, meaning that the brain represents probability distributions over world states. However, despite the apparent simplicity of this statement, there are substantial disagreements on exactly what probabilistic perception is and how it should be tested. In this adversarial collaboration, two proponents (Jehee and Denison) and two skeptics (Rahnev and Block) of probabilistic perception deliberate on the terms of the debate and present arguments for or against the notion that perception is probabilistic. We believe that this collaboration helps clarify the critical issues that need to be considered but that further work is required to reach consensus.},
	language = {en},
	urldate = {2022-02-07},
	institution = {PsyArXiv},
	author = {Rahnev, Dobromir and Block, Ned and Denison, Rachel N. and Jehee, Janneke},
	month = may,
	year = {2021},
	doi = {10.31234/osf.io/f8v5r},
}

@article{block_if_2018,
	title = {If perception is probabilistic, why does it not seem probabilistic?},
	volume = {373},
	issn = {0962-8436, 1471-2970},
	url = {https://royalsocietypublishing.org/doi/10.1098/rstb.2017.0341},
	doi = {10.1098/rstb.2017.0341},
	abstract = {The success of the Bayesian perspective in explaining perceptual phenomena has motivated the view that perceptual representation is probabilistic. But if perceptual representation is probabilistic, why does normal conscious perception not reflect the full probability functions that the probabilistic point of view endorses? For example, neurons in cortical area MT that respond to the direction of motion are broadly tuned: a patch of cortex that is tuned to vertical motion also responds to horizontal motion, but when we see vertical motion, foveally, in good conditions, it does not look at all horizontal. The standard solution in terms of sampling runs into the problem that sampling is an account of perceptual decision rather than perception. This paper argues that the best Bayesian approach to this problem does not require probabilistic representation.
            This article is part of the theme issue ‘Perceptual consciousness and cognitive access'.},
	language = {en},
	number = {1755},
	urldate = {2022-02-07},
	journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
	author = {Block, Ned},
	month = sep,
	year = {2018},
	pages = {20170341},
}

@misc{noauthor_confusion_nodate,
	title = {{ON} {A} {CONFUSION} {ABOUT} {A} {FUNCTION} {OF} {CONSCIOUSNESS}},
	url = {http://cogprints.org/231/1/199712004.html},
	urldate = {2022-02-07},
}

@article{barraclough_prefrontal_2004,
	title = {Prefrontal cortex and decision making in a mixed-strategy game},
	volume = {7},
	issn = {1097-6256, 1546-1726},
	url = {http://www.nature.com/articles/nn1209},
	doi = {10.1038/nn1209},
	language = {en},
	number = {4},
	urldate = {2022-02-04},
	journal = {Nature Neuroscience},
	author = {Barraclough, Dominic J and Conroy, Michelle L and Lee, Daeyeol},
	month = apr,
	year = {2004},
	pages = {404--410},
}

@article{kuchibhotla_parallel_2017,
	title = {Parallel processing by cortical inhibition enables context-dependent behavior},
	volume = {20},
	issn = {1097-6256},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5191967/},
	doi = {10.1038/nn.4436},
	abstract = {Physical features of sensory stimuli are fixed, but sensory perception is context-dependent. The precise mechanisms that govern contextual modulation remain unknown. Here, we trained mice to switch between two contexts: passively listening to pure tones vs. performing a recognition task for the same stimuli. Two-photon imaging showed that many excitatory neurons in auditory cortex were suppressed, while some cells became more active during behavior. Whole-cell recordings showed that excitatory inputs were only modestly affected by context, but inhibition was more sensitive, with PV, SOM+, and VIP+ interneurons balancing inhibition/disinhibition within the network. Cholinergic modulation was involved in context-switching, with cholinergic axons increasing activity during behavior and directly depolarizing inhibitory cells. Network modeling captured these findings, but only when modulation coincidently drove all three interneuron subtypes, ruling out either inhibition or disinhibition alone as sole mechanism for active engagement. Parallel processing of cholinergic modulation by cortical interneurons therefore enables context-dependent behavior.},
	number = {1},
	urldate = {2022-02-03},
	journal = {Nature neuroscience},
	author = {Kuchibhotla, Kishore V. and Gill, Jonathan V. and Lindsay, Grace W. and Papadoyannis, Eleni S. and Field, Rachel E. and Hindmarsh Sten, Tom A. and Miller, Kenneth D. and Froemke, Robert C.},
	month = jan,
	year = {2017},
	pmid = {27798631},
	pmcid = {PMC5191967},
	pages = {62--71},
}

@article{kuchibhotla_synchronous_2009,
	title = {Synchronous {Hyperactivity} and {Intercellular} {Calcium} {Waves} in {Astrocytes} in {Alzheimer} {Mice}},
	volume = {323},
	issn = {0036-8075},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2884172/},
	doi = {10.1126/science.1169096},
	abstract = {While senile plaques injure neurons focally, the functional response of astrocytes to Alzheimer’s disease pathology is unknown. Using multiphoton fluorescence lifetime imaging microscopy in vivo, we quantitatively imaged astrocytic calcium homeostasis in a mouse model of Alzheimer’s disease. Resting calcium was globally elevated in the astrocytic network, but was independent of proximity to individual plaques. Time lapse imaging revealed that calcium transients in astrocytes were more frequent, were synchronously coordinated across long distances, and were uncoupled from neuronal activity. Furthermore, rare intercellular calcium waves were observed, but only in mice with amyloid-β plaques, originating near plaques and spreading radially at least 200 µm. Thus, while neurotoxicity is observed near amyloid-β deposits, there exists a more general astrocyte-based network response to focal pathology.},
	number = {5918},
	urldate = {2022-02-03},
	journal = {Science (New York, N.Y.)},
	author = {Kuchibhotla, Kishore V. and Lattarulo, Carli R. and Hyman, Bradley T. and Bacskai, Brian J.},
	month = feb,
	year = {2009},
	pmid = {19251629},
	pmcid = {PMC2884172},
	pages = {1211--1215},
}

@article{noauthor_reading_nodate,
	title = {Reading {Modality} {Modifies} {Reading} {Network}: {Insights} from {Neural} basis of {Braille} in {Proficient} {Blind} {Readers}},
	language = {en},
	pages = {59},
}

@article{thompson-schill_frontal_2005,
	title = {The frontal lobes and the regulation of mental activity},
	volume = {15},
	issn = {09594388},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0959438805000383},
	doi = {10.1016/j.conb.2005.03.006},
	language = {en},
	number = {2},
	urldate = {2022-02-03},
	journal = {Current Opinion in Neurobiology},
	author = {Thompson-Schill, Sharon L and Bedny, Marina and Goldberg, Robert F},
	month = apr,
	year = {2005},
	pages = {219--224},
}

@article{anderson_reflections_1991,
	title = {Reflections of the {Environment} in {Memory}},
	volume = {2},
	issn = {0956-7976, 1467-9280},
	url = {http://journals.sagepub.com/doi/10.1111/j.1467-9280.1991.tb00174.x},
	doi = {10.1111/j.1467-9280.1991.tb00174.x},
	abstract = {Availability of human memories for specific items shows reliable relationships to frequency, recency, and pattern ofprior exposures to the item. These relationships have defied a systematic theoretical treatment. A number ofenvironmental sources (New York Times, parental speech, electronic mail) are examined to show that the probability that a memory will be needed also shows reliable relationships tofrequency, recency, and pattern of prior exposures. Moreover, the environmental relationships are the same as the memory relationships. It is argued that human memory has the form it does because it is adapted to these environmental relationships. Models for both the environment and human memory are described. Among the memory phenomena addressed are the practice function, the retentioll functioll, the effect of spacing of practice, and the relationship between degree ofpractice and retention.},
	language = {en},
	number = {6},
	urldate = {2022-02-02},
	journal = {Psychological Science},
	author = {Anderson, John R. and Schooler, Lael J.},
	month = nov,
	year = {1991},
	pages = {396--408},
}

@article{evans_direct_2015,
	title = {Direct {Real}-{Time} {Neural} {Evidence} for {Task}-{Set} {Inertia}},
	volume = {26},
	issn = {0956-7976, 1467-9280},
	url = {http://journals.sagepub.com/doi/10.1177/0956797614561799},
	doi = {10.1177/0956797614561799},
	abstract = {One influential explanation for the costs incurred when switching between tasks is that they reflect interference arising from completing the previous task—known as task-set inertia. We report a novel approach for assessing task-set inertia in a memory experiment using event-related potentials (ERPs). After a study phase, participants completed a test block in which they switched between a memory task (retrieving information from the study phase) and a perceptual task. These tasks alternated every two trials. An ERP index of the retrieval of study information was evident in the memory task. It was also present on the first trial of the perceptual task but was markedly attenuated on the second. Moreover, this task-irrelevant ERP activity was positively correlated with a behavioral cost associated with switching between tasks. This real-time measure of neural activity thus provides direct evidence of task-set inertia, its duration, and the functional role it plays in switch costs.},
	language = {en},
	number = {3},
	urldate = {2022-02-02},
	journal = {Psychological Science},
	author = {Evans, Lisa H. and Herron, Jane E. and Wilding, Edward L.},
	month = mar,
	year = {2015},
	pages = {284--290},
}

@article{mahto_multi-timescale_2021,
	title = {Multi-timescale {Representation} {Learning} in {LSTM} {Language} {Models}},
	url = {http://arxiv.org/abs/2009.12727},
	abstract = {Language models must capture statistical dependencies between words at timescales ranging from very short to very long. Earlier work has demonstrated that dependencies in natural language tend to decay with distance between words according to a power law. However, it is unclear how this knowledge can be used for analyzing or designing neural network language models. In this work, we derived a theory for how the memory gating mechanism in long short-term memory (LSTM) language models can capture power law decay. We found that unit timescales within an LSTM, which are determined by the forget gate bias, should follow an Inverse Gamma distribution. Experiments then showed that LSTM language models trained on natural English text learn to approximate this theoretical distribution. Further, we found that explicitly imposing the theoretical distribution upon the model during training yielded better language model perplexity overall, with particular improvements for predicting low-frequency (rare) words. Moreover, the explicit multi-timescale model selectively routes information about different types of words through units with different timescales, potentially improving model interpretability. These results demonstrate the importance of careful, theoretically-motivated analysis of memory and timescale in language models.},
	urldate = {2022-02-02},
	journal = {arXiv:2009.12727 [cs]},
	author = {Mahto, Shivangi and Vo, Vy A. and Turek, Javier S. and Huth, Alexander G.},
	month = mar,
	year = {2021},
	note = {arXiv: 2009.12727},
	keywords = {91F20, Computer Science - Computation and Language, Computer Science - Machine Learning, I.2.6, I.2.7},
}

@article{bilandzic_narrative_2019,
	title = {The {Narrative} {Engageability} {Scale}: {A} {Multidimensional} {Trait} {Measure} for the {Propensity} to {Become} {Engaged} in a {Story}},
	language = {en},
	author = {Bilandzic, Helena},
	year = {2019},
	pages = {32},
}

@article{macleod_zeigarnik_2020,
	title = {Zeigarnik and von {Restorff}: {The} memory effects and the stories behind them},
	volume = {48},
	issn = {1532-5946},
	shorttitle = {Zeigarnik and von {Restorff}},
	url = {https://doi.org/10.3758/s13421-020-01033-5},
	doi = {10.3758/s13421-020-01033-5},
	abstract = {Two of the best known eponymous phenomena in memory research were carried out as dissertations in the same era at the same university, each supervised by an influential researcher working within the Gestalt framework. Both examined the influence of unexpected events on memory. Bluma Zeigarnik (Psychologische Forschung, 9, 1–85, 1927) first reported that memory is better for interrupted tasks than for completed tasks, a phenomenon long known as the Zeigarnik effect. Hedwig von Restorff (Psychologische Forschung, 18, 299–342, 1933) first reported that memory is better for isolated than for non-isolated pieces of information, a phenomenon long known as the von Restorff effect. In this article, I present: (1) a biographical sketch of the researcher behind each phenomenon, (2) a description of their dissertation research, and (3) an evaluation of the current status of each phenomenon.},
	language = {en},
	number = {6},
	urldate = {2022-02-02},
	journal = {Memory \& Cognition},
	author = {MacLeod, Colin M.},
	month = aug,
	year = {2020},
	pages = {1073--1088},
}

@article{ophir_cognitive_2009,
	title = {Cognitive control in media multitaskers},
	volume = {106},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/lookup/doi/10.1073/pnas.0903620106},
	doi = {10.1073/pnas.0903620106},
	abstract = {Chronic media multitasking is quickly becoming ubiquitous, although processing multiple incoming streams of information is considered a challenge for human cognition. A series of experiments addressed whether there are systematic differences in information processing styles between chronically heavy and light media multitaskers. A trait media multitasking index was developed to identify groups of heavy and light media multitaskers. These two groups were then compared along established cognitive control dimensions. Results showed that heavy media multitaskers are more susceptible to interference from irrelevant environmental stimuli and from irrelevant representations in memory. This led to the surprising result that heavy media multitaskers performed worse on a test of task-switching ability, likely due to reduced ability to filter out interference from the irrelevant task set. These results demonstrate that media multitasking, a rapidly growing societal trend, is associated with a distinct approach to fundamental information processing.},
	language = {en},
	number = {37},
	urldate = {2022-02-02},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Ophir, Eyal and Nass, Clifford and Wagner, Anthony D.},
	month = sep,
	year = {2009},
	pages = {15583--15587},
}

@article{poldrack_physics_2021,
	title = {The physics of representation},
	volume = {199},
	issn = {1573-0964},
	url = {https://doi.org/10.1007/s11229-020-02793-y},
	doi = {10.1007/s11229-020-02793-y},
	abstract = {The concept of “representation” is used broadly and uncontroversially throughout neuroscience, in contrast to its highly controversial status within the philosophy of mind and cognitive science. In this paper I first discuss the way that the term is used within neuroscience, in particular describing the strategies by which representations are characterized empirically. I then relate the concept of representation within neuroscience to one that has developed within the field of machine learning (in particular through recent work in deep learning or “representation learning”). I argue that the recent success of artificial neural networks on certain tasks such as visual object recognition reflects the degree to which those systems (like biological brains) exhibit inherent inductive biases that reflect the structure of the physical world. I further argue that any system that is going to behave intelligently in the world must contain representations that reflect the structure of the world; otherwise, the system must perform unconstrained function approximation which is destined to fail due to the curse of dimensionality, in which the number of possible states of the world grows exponentially with the number of dimensions in the space of possible inputs. An analysis of these concepts in light of philosophical debates regarding the ontological status of representations suggests that the representations identified within both biological and artificial neural networks qualify as legitimate representations in the philosophical sense.},
	language = {en},
	number = {1},
	urldate = {2022-02-01},
	journal = {Synthese},
	author = {Poldrack, Russell A.},
	month = dec,
	year = {2021},
	pages = {1307--1325},
}

@article{aron_inhibition_2004,
	title = {Inhibition and the right inferior frontal cortex},
	volume = {8},
	issn = {13646613},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1364661304000531},
	doi = {10.1016/j.tics.2004.02.010},
	language = {en},
	number = {4},
	urldate = {2022-02-01},
	journal = {Trends in Cognitive Sciences},
	author = {Aron, Adam R. and Robbins, Trevor W. and Poldrack, Russell A.},
	month = apr,
	year = {2004},
	pages = {170--177},
}

@article{keller_attention_2022,
	title = {Attention enhances category representations across the brain with strengthened residual correlations to ventral temporal cortex},
	volume = {249},
	issn = {1053-8119},
	url = {https://www.sciencedirect.com/science/article/pii/S1053811922000301},
	doi = {10.1016/j.neuroimage.2022.118900},
	abstract = {How does attention enhance neural representations of goal-relevant stimuli while suppressing representations of ignored stimuli across regions of the brain? While prior studies have shown that attention enhances visual responses, we lack a cohesive understanding of how selective attention modulates visual representations across the brain. Here, we used functional magnetic resonance imaging (fMRI) while participants performed a selective attention task on superimposed stimuli from multiple categories and used a data-driven approach to test how attention affects both decodability of category information and residual correlations (after regressing out stimulus-driven variance) with category-selective regions of ventral temporal cortex (VTC). Our data reveal three main findings. First, when two objects are simultaneously viewed, the category of the attended object can be decoded more readily than the category of the ignored object, with the greatest attentional enhancements observed in occipital and temporal lobes. Second, after accounting for the response to the stimulus, the correlation in the residual brain activity between a cortical region and a category-selective region of VTC was elevated when that region's preferred category was attended vs. ignored, and more so in the right occipital, parietal, and frontal cortices. Third, we found that the stronger the residual correlations between a given region of cortex and VTC, the better visual category information could be decoded from that region. These findings suggest that heightened residual correlations by selective attention may reflect the sharing of information between sensory regions and higher-order cortical regions to provide attentional enhancement of goal-relevant information.},
	language = {en},
	urldate = {2022-02-01},
	journal = {NeuroImage},
	author = {Keller, Arielle S. and Jagadeesh, Akshay V. and Bugatus, Lior and Williams, Leanne M. and Grill-Spector, Kalanit},
	month = apr,
	year = {2022},
	keywords = {Bottom up processing, Selective attention, Top down, Visual object categories, fMRI},
	pages = {118900},
}

@article{grill-spector_repetition_2006,
	title = {Repetition and the brain: neural models of stimulus-specific effects},
	volume = {10},
	issn = {13646613},
	shorttitle = {Repetition and the brain},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1364661305003232},
	doi = {10.1016/j.tics.2005.11.006},
	language = {en},
	number = {1},
	urldate = {2022-02-01},
	journal = {Trends in Cognitive Sciences},
	author = {Grill-Spector, Kalanit and Henson, Richard and Martin, Alex},
	month = jan,
	year = {2006},
	pages = {14--23},
}

@article{cao_explanatory_2021,
	title = {Explanatory models in neuroscience: {Part} 1 -- taking mechanistic abstraction seriously},
	shorttitle = {Explanatory models in neuroscience},
	url = {http://arxiv.org/abs/2104.01490},
	abstract = {Despite the recent success of neural network models in mimicking animal performance on visual perceptual tasks, critics worry that these models fail to illuminate brain function. We take it that a central approach to explanation in systems neuroscience is that of mechanistic modeling, where understanding the system is taken to require ﬂeshing out the parts, organization, and activities of a system, and how those give rise to behaviors of interest. However, it remains somewhat controversial what it means for a model to describe a mechanism, and whether neural network models qualify as explanatory.},
	language = {en},
	urldate = {2022-02-01},
	journal = {arXiv:2104.01490 [cs, q-bio]},
	author = {Cao, Rosa and Yamins, Daniel},
	month = apr,
	year = {2021},
	note = {arXiv: 2104.01490},
	keywords = {Computer Science - Neural and Evolutionary Computing, Quantitative Biology - Neurons and Cognition},
}

@article{bonnen_when_nodate,
	title = {When the ventral visual stream is not enough: {A} deep learning account of medial temporal lobe involvement in perception},
	language = {en},
	author = {Bonnen, Tyler and Yamins, Daniel L K and Wagner, Anthony D},
	pages = {28},
}

@article{cao_explanatory_2021-1,
	title = {Explanatory models in neuroscience: {Part} 2 -- constraint-based intelligibility},
	shorttitle = {Explanatory models in neuroscience},
	url = {http://arxiv.org/abs/2104.01489},
	abstract = {Computational modeling plays an increasingly important role in neuroscience, highlighting the philosophical question of how computational models explain. In the context of neural network models for neuroscience, concerns have been raised about model intelligibility, and how they relate (if at all) to what is found in the brain. We claim that what makes a system intelligible is an understanding of the dependencies between its behavior and the factors that are causally responsible for that behavior. In biological systems, many of these dependencies are naturally “top-down”: ethological imperatives interact with evolutionary and developmental constraints under natural selection. We describe how the optimization techniques used to construct neural network models capture some key aspects of these dependencies, and thus help explain why brain systems are as they are – because when a challenging ecologically-relevant goal is shared by a neural network and the brain, it places tight constraints on the possible mechanisms exhibited in both kinds of systems. The presence and strength of these constraints explain why some outcomes are more likely than others. By combining two familiar modes of explanation – one based on bottom-up mechanism (whose relation to neural network models we address in a companion paper) and the other on top-down constraints, these models can illuminate brain function.},
	language = {en},
	urldate = {2022-02-01},
	journal = {arXiv:2104.01489 [cs, q-bio]},
	author = {Cao, Rosa and Yamins, Daniel},
	month = apr,
	year = {2021},
	note = {arXiv: 2104.01489},
	keywords = {Computer Science - Neural and Evolutionary Computing, Quantitative Biology - Neurons and Cognition},
}

@article{gardner_population_2021,
	title = {Population {Models}, {Not} {Analyses}, of {Human} {Neuroscience} {Measurements}},
	volume = {7},
	issn = {2374-4642, 2374-4650},
	url = {https://www.annualreviews.org/doi/10.1146/annurev-vision-093019-111124},
	doi = {10.1146/annurev-vision-093019-111124},
	abstract = {Selectivity for many basic properties of visual stimuli, such as orientation, is thought to be organized at the scale of cortical columns, making it difficult or impossible to measure directly with noninvasive human neuroscience measurement. However, computational analyses of neuroimaging data have shown that selectivity for orientation can be recovered by considering the pattern of response across a region of cortex. This suggests that computational analyses can reveal representation encoded at a finer spatial scale than is implied by the spatial resolution limits of measurement techniques. This potentially opens up the possibility to study a much wider range of neural phenomena that are otherwise inaccessible through noninvasive measurement. However, as we review in this article, a large body of evidence suggests an alternative hypothesis to this superresolution account: that orientation information is available at the spatial scale of cortical maps and thus easily measurable at the spatial resolution of standard techniques. In fact, a population model shows that this orientation information need not even come from single-unit selectivity for orientation tuning, but instead can result from population selectivity for spatial frequency. Thus, a categorical error of interpretation can result whereby orientation selectivity can be confused with spatial frequency selectivity. This is similarly problematic for the interpretation of results from numerous studies of more complex representations and cognitive functions that have built upon the computational techniques used to reveal stimulus orientation. We suggest in this review that these interpretational ambiguities can be avoided by treating computational analyses as models of the neural processes that give rise to measurement. Building upon the modeling tradition in vision science using considerations of whether population models meet a set of core criteria is important for creating the foundation for a cumulative and replicable approach to making valid inferences from human neuroscience measurements.},
	language = {en},
	number = {1},
	urldate = {2022-01-31},
	journal = {Annual Review of Vision Science},
	author = {Gardner, Justin L. and Merriam, Elisha P.},
	month = sep,
	year = {2021},
	pages = {225--255},
}

@article{kong_increasing_2022,
	title = {Increasing neural network robustness improves match to macaque {V1} eigenspectrum, spatial frequency preference and predictivity},
	volume = {18},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1009739},
	doi = {10.1371/journal.pcbi.1009739},
	abstract = {Task-optimized convolutional neural networks (CNNs) show striking similarities to the ventral visual stream. However, human-imperceptible image perturbations can cause a CNN to make incorrect predictions. Here we provide insight into this brittleness by investigating the representations of models that are either robust or not robust to image perturbations. Theory suggests that the robustness of a system to these perturbations could be related to the power law exponent of the eigenspectrum of its set of neural responses, where power law exponents closer to and larger than one would indicate a system that is less susceptible to input perturbations. We show that neural responses in mouse and macaque primary visual cortex (V1) obey the predictions of this theory, where their eigenspectra have power law exponents of at least one. We also find that the eigenspectra of model representations decay slowly relative to those observed in neurophysiology and that robust models have eigenspectra that decay slightly faster and have higher power law exponents than those of non-robust models. The slow decay of the eigenspectra suggests that substantial variance in the model responses is related to the encoding of fine stimulus features. We therefore investigated the spatial frequency tuning of artificial neurons and found that a large proportion of them preferred high spatial frequencies and that robust models had preferred spatial frequency distributions more aligned with the measured spatial frequency distribution of macaque V1 cells. Furthermore, robust models were quantitatively better models of V1 than non-robust models. Our results are consistent with other findings that there is a misalignment between human and machine perception. They also suggest that it may be useful to penalize slow-decaying eigenspectra or to bias models to extract features of lower spatial frequencies during task-optimization in order to improve robustness and V1 neural response predictivity.},
	language = {en},
	number = {1},
	urldate = {2022-01-29},
	journal = {PLOS Computational Biology},
	author = {Kong, Nathan C. L. and Margalit, Eshed and Gardner, Justin L. and Norcia, Anthony M.},
	month = jan,
	year = {2022},
	note = {Publisher: Public Library of Science},
	keywords = {Electrophysiology, Macaque, Neuronal tuning, Neurons, Perturbation theory, Vision, Visual cortex, Visual system},
	pages = {e1009739},
}

@techreport{gerstenberg_what_2022,
	title = {What would have happened? {Counterfactuals}, hypotheticals, and causal judgments},
	shorttitle = {What would have happened?},
	url = {https://psyarxiv.com/rsb46/},
	abstract = {How do people make causal judgments? In this paper, I show that counterfactuals are necessary for explaining causal judgments about  events, and that hypotheticals don't suffice. In two experiments, participants viewed video clips of dynamic interactions between billiard balls. In Experiment 1, participants either made hypothetical judgments about whether ball B would go through the gate if ball A weren't present in the scene, or counterfactual judgments about whether ball B would have gone through the gate if ball A hadn't been present. Because the clips featured a block in front of the gate that sometimes moved and sometimes stayed put, hypothetical and counterfactual judgments came apart. A computational model that evaluates hypotheticals and counterfactuals by running noisy physical simulations accurately captured participants' judgments. In Experiment 2, participants judged whether ball A caused ball B to go through the gate. The results showed a tight fit between counterfactual and causal judgments, whereas hypothetical judgments didn't predict causal judgments. I discuss the implications of this work for theories of causality, and for studying the development of counterfactual thinking in children.},
	language = {en-us},
	urldate = {2022-01-28},
	institution = {PsyArXiv},
	author = {Gerstenberg, Tobias},
	month = jan,
	year = {2022},
	doi = {10.31234/osf.io/rsb46},
	note = {type: article},
	keywords = {Cognitive Psychology, Imagery, Reasoning, Social and Behavioral Sciences, causality, conditional, counterfactual, hypothetical, intuitive physics, mental simulation},
}

@inproceedings{bender_climbing_2020,
	address = {Online},
	title = {Climbing towards {NLU}: {On} {Meaning}, {Form}, and {Understanding} in the {Age} of {Data}},
	shorttitle = {Climbing towards {NLU}},
	url = {https://www.aclweb.org/anthology/2020.acl-main.463},
	doi = {10.18653/v1/2020.acl-main.463},
	abstract = {The success of the large neural language models on many NLP tasks is exciting. However, we ﬁnd that these successes sometimes lead to hype in which these models are being described as “understanding” language or capturing “meaning”. In this position paper, we argue that a system trained only on form has a priori no way to learn meaning. In keeping with the ACL 2020 theme of “Taking Stock of Where We’ve Been and Where We’re Going”, we argue that a clear understanding of the distinction between form and meaning will help guide the ﬁeld towards better science around natural language understanding.},
	language = {en},
	urldate = {2022-01-28},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Bender, Emily M. and Koller, Alexander},
	year = {2020},
	pages = {5185--5198},
}

@inproceedings{sennrich_neural_2016,
	address = {Berlin, Germany},
	title = {Neural {Machine} {Translation} of {Rare} {Words} with {Subword} {Units}},
	url = {http://aclweb.org/anthology/P16-1162},
	doi = {10.18653/v1/P16-1162},
	abstract = {Neural machine translation (NMT) models typically operate with a ﬁxed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character ngram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English→German and English→Russian by up to 1.1 and 1.3 BLEU, respectively.},
	language = {en},
	urldate = {2022-01-27},
	booktitle = {Proceedings of the 54th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
	year = {2016},
	pages = {1715--1725},
}

@article{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be ﬁnetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspeciﬁc architecture modiﬁcations.},
	language = {en},
	urldate = {2022-01-27},
	journal = {arXiv:1810.04805 [cs]},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {arXiv: 1810.04805},
	keywords = {Computer Science - Computation and Language},
}

@techreport{momennejad_predicting_2018,
	type = {preprint},
	title = {Predicting the {Future} with {Multi}-scale {Successor} {Representations}},
	url = {http://biorxiv.org/lookup/doi/10.1101/449470},
	abstract = {The successor representation (SR) is a candidate principle for generalization in reinforcement learning, computational accounts of memory, and the structure of neural representations in the hippocampus. Given a sequence of states, the SR learns a predictive representation for every given state that encodes how often, on average, each upcoming state is expected to be visited, even if it is multiple steps ahead. A discount or scale parameter determines how many steps into the future SR’s generalizations reach, enabling rapid value computation, subgoal discovery, and ﬂexible decision-making in large trees. However, SR with a single scale could discard information for predicting both the sequential order of and the distance between states, which are common problems in navigation for animals and artiﬁcial agents. Here we propose a solution: an ensemble of SRs with multiple scales. We show that the derivative of multi-scale SR can reconstruct both the sequence of expected future states and estimate distance to goal. This derivative can be computed linearly: we show that a multi-scale SR ensemble is the Laplace transform of future states, and the inverse of this Laplace transform is a biologically plausible linear estimation of the derivative. Multi-scale SR and its derivative could lead to a common principle for how the medial temporal lobe supports both map-based and vector-based navigation.},
	language = {en},
	urldate = {2022-01-25},
	institution = {Neuroscience},
	author = {Momennejad, Ida and Howard, Marc W.},
	month = oct,
	year = {2018},
	doi = {10.1101/449470},
	keywords = {honey},
}

@article{jacques_deep_2021,
	title = {A deep convolutional neural network that is invariant to time rescaling},
	url = {http://arxiv.org/abs/2107.04616},
	abstract = {Human learners can readily understand speech, or a melody, when it is presented slower or faster than usual. Although deep convolutional neural networks (CNNs) are extremely powerful in extracting information from time series, they require explicit training to generalize to different time scales. This paper presents a deep CNN that incorporates a temporal representation inspired by recent ﬁndings from neuroscience. In the mammalian brain, time is represented by populations of neurons with temporal receptive ﬁelds. Critically, the peaks of the receptive ﬁelds form a geometric series, such that the population codes a set of temporal basis functions over log time. Because memory for the recent past is a function of log time, rescaling the input results in translation of the memory. The Scale-Invariant Temporal History Convolution network (SITHCon) builds a convolutional layer over this logarithmically-distributed temporal memory. A max-pool operation results in a network that is invariant to rescalings of time modulo edge effects. We compare performance of SITHCon to a Temporal Convolution Network (TCN). Although both networks can learn classiﬁcation and regression problems on both univariate and multivariate time series f (t), only SITHCon generalizes to rescalings f (at). This property, inspired by ﬁndings from contemporary neuroscience and consistent with ﬁndings from cognitive psychology, may enable networks that learn with fewer training examples, fewer weights and that generalize more robustly to out of sample data.},
	language = {en},
	urldate = {2022-01-24},
	journal = {arXiv:2107.04616 [cs]},
	author = {Jacques, Brandon G. and Tiganj, Zoran and Sarkar, Aakash and Howard, Marc W. and Sederberg, Per B.},
	month = oct,
	year = {2021},
	note = {arXiv: 2107.04616},
	keywords = {Computer Science - Machine Learning, honey, read},
}

@inproceedings{torralba_statistical_2001,
	title = {Statistical context priming for object detection},
	volume = {1},
	doi = {10.1109/ICCV.2001.937604},
	abstract = {There is general consensus that context can be a rich source of information about an object's identity, location and scale. However the issue of how to formalize centextual influences is still largely open. Here we introduce a simple probabilistic framework for modeling the relationship between context and object properties. We represent global context information in terms of the spatial layout of spectral components. The resulting scheme serves as an effective procedure for context driven focus of attention and scale-selection on real-world scenes. Based on a simple holistic analysis of an image, the scheme is able to accurately predict object locations and sizes.},
	booktitle = {Proceedings {Eighth} {IEEE} {International} {Conference} on {Computer} {Vision}. {ICCV} 2001},
	author = {Torralba, A. and Sinha, P.},
	month = jul,
	year = {2001},
	keywords = {Cognitive science, Context modeling, Focusing, Image analysis, Image recognition, Information resources, Layout, Marine vehicles, Object detection, Visual system, honey},
	pages = {763--770 vol.1},
}

@article{tiganj_computational_nodate,
	title = {A computational model for simulating the future using a memory timeline},
	abstract = {The ability to learn temporal relationships and use that knowledge to simulate future events is among the most remarkable aspects of cognition. Recently introduced behavioral task called Judgment of Imminence (JOI) combined with a wellknown Judgment of Recency (JOR) task pointed to a remarkable symmetry between the temporal organization of memory and prediction. The data were consistent with the hypothesis that both memory and prediction can be organized as a compressed mental timeline. This means that the past and future can be remembered or simulated sequentially relative to the present. The compression implies that events closer to the present, regardless of whether they are in the past or in the future, were represented more accurately than those further from the present. Here we used the existing JOR model based on a compressed memory timeline to build an associative representation that can learn the temporal relationships and create a timeline of the future, which mirrors the timeline of the past. We show that this approach can simultaneously account for response times and accuracy in both JOR and JOI. This work provides a time-local neural-level mechanistic account for how the temporal organization of the memory can be used to learn the temporal structure of the world and simulate the future in an efﬁcient manner as a compressed mental timeline.},
	language = {en},
	author = {Tiganj, Zoran and Tang, Wei and Howard, Marc W},
	keywords = {honey},
	pages = {8},
}

@article{howard_distributed_2002,
	title = {A {Distributed} {Representation} of {Temporal} {Context}},
	volume = {46},
	issn = {00222496},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0022249601913884},
	doi = {10.1006/jmps.2001.1388},
	language = {en},
	number = {3},
	urldate = {2022-01-24},
	journal = {Journal of Mathematical Psychology},
	author = {Howard, Marc W. and Kahana, Michael J.},
	month = jun,
	year = {2002},
	keywords = {honey},
	pages = {269--299},
}

@article{duncan_memory_2016,
	title = {Memory states influence value-based decisions.},
	volume = {145},
	issn = {1939-2222, 0096-3445},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/xge0000231},
	doi = {10.1037/xge0000231},
	abstract = {Using memory to guide decisions allows past experience to improve future outcomes. However, the circumstances that modulate how and when memory influences decisions are not well understood. Here, we report that the use of memories to guide decisions depends on the context in which these decisions are made. We show that decisions made in the context of familiar images are more likely to be influenced by past events than decisions made in the context of novel images (Exp 1), that this bias persists even when a temporal gap is introduced between the image presentation and the decision (Exp 2), and that contextual novelty facilitates value-learning whereas familiarity facilitates the retrieval and use of previously learned values (Exp 3). These effects are consistent with neurobiological and computational models of memory, which propose that familiar images evoke a lingering ‘retrieval state’ that facilitates the recollection of other episodic memories. Together, these experiments highlight the importance of episodic memory for decision-making and provides an example of how computational and neurobiological theories can lead to new insights into how and when different types of memories guide our choices.},
	language = {en},
	number = {11},
	urldate = {2022-01-24},
	journal = {Journal of Experimental Psychology: General},
	author = {Duncan, Katherine D. and Shohamy, Daphna},
	month = nov,
	year = {2016},
	keywords = {honey},
	pages = {1420--1426},
}

@techreport{bellana_narrative_2021,
	title = {Narrative thinking lingers in spontaneous thought},
	url = {https://psyarxiv.com/gxzyj/},
	abstract = {What we think about at any moment is shaped by what preceded it. Why do some experiences, such as reading an immersive story, feel as if they linger in mind for longer than others? In this study, we hypothesize that the stream of our thinking is especially affected by "deeper" forms of processing, emphasizing the meaning and implications of a stimulus rather than its immediate physical properties or low-level semantics (e.g., reading a story vs. reading disconnected words). To test this idea, we presented participants with short stories that preserved different levels of coherence (word-level, sentence-level, or intact narrative), and we measured participants’ self-reports of lingering and spontaneous word generation. Participants reported that stories lingered in their minds after reading, but this effect was greatly reduced when the same words were read with sentence or word-order randomly shuffled. Furthermore, the words that participants spontaneously generated after reading shared semantic meaning with the story’s central themes, particularly when the story was coherent (i.e., intact). Crucially, regardless of the objective coherence of what each participant read, lingering was strongest amongst participants who reported being ‘transported’ into the world of the story while reading. We further generalized this result to a non-narrative stimulus, finding that participants reported lingering after reading a list of words, especially when they had sought an underlying narrative or theme across words. We conclude that recent experiences are most likely to exert a lasting mental context when we seek to extract and represent their deep situation-level meaning.},
	language = {en-us},
	urldate = {2022-01-17},
	institution = {PsyArXiv},
	author = {Bellana, Buddhika and Mahabal, Abhijit and Honey, Christopher J.},
	month = aug,
	year = {2021},
	doi = {10.31234/osf.io/gxzyj},
	note = {type: article},
	keywords = {Cognitive Psychology, Memory, Social and Behavioral Sciences, deep processing, honey, memory, mental context, mind wandering, narratives},
}

@article{bransford_contextual_1972,
	title = {Contextual prerequisites for understanding: {Some} investigations of comprehension and recall},
	volume = {11},
	issn = {00225371},
	shorttitle = {Contextual prerequisites for understanding},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0022537172800069},
	doi = {10.1016/S0022-5371(72)80006-9},
	language = {en},
	number = {6},
	urldate = {2022-01-24},
	journal = {Journal of Verbal Learning and Verbal Behavior},
	author = {Bransford, John D. and Johnson, Marcia K.},
	month = dec,
	year = {1972},
	keywords = {honey},
	pages = {717--726},
}

@misc{noauthor_task-optimized_nodate,
	title = {A {Task}-{Optimized} {Neural} {Network} {Replicates} {Human} {Auditory} {Behavior}, {Predicts} {Brain} {Responses}, and {Reveals} a {Cortical} {Processing} {Hierarchy} {\textbar} {Elsevier} {Enhanced} {Reader}},
	url = {https://reader.elsevier.com/reader/sd/pii/S0896627318302502?token=956F02F82EF418CFD765D8EA1001166C6D2D97C1EBE336CC8A8530D747E668C6D95D7F97EA4AED9DE3B084758F90A6BB&originRegion=eu-west-1&originCreation=20220112105950},
	language = {en},
	urldate = {2022-01-12},
	doi = {10.1016/j.neuron.2018.03.044},
}

@article{linzen_assessing_2016,
	title = {Assessing the {Ability} of {LSTMs} to {Learn} {Syntax}-{Sensitive} {Dependencies}},
	volume = {4},
	issn = {2307-387X},
	url = {https://direct.mit.edu/tacl/article/43378},
	doi = {10.1162/tacl_a_00115},
	abstract = {The success of long short-term memory (LSTM) neural networks in language processing is typically attributed to their ability to capture long-distance statistical regularities. Linguistic regularities are often sensitive to syntactic structure; can such dependencies be captured by LSTMs, which do not have explicit structural representations? We begin addressing this question using number agreement in English subject-verb dependencies. We probe the architecture’s grammatical competence both using training objectives with an explicit grammatical target (number prediction, grammaticality judgments) and using language models. In the strongly supervised settings, the LSTM achieved very high overall accuracy (less than 1\% errors), but errors increased when sequential and structural information conﬂicted. The frequency of such errors rose sharply in the language-modeling setting. We conclude that LSTMs can capture a non-trivial amount of grammatical structure given targeted supervision, but stronger architectures may be required to further reduce errors; furthermore, the language modeling signal is insufﬁcient for capturing syntax-sensitive dependencies, and should be supplemented with more direct supervision if such dependencies need to be captured.},
	language = {en},
	urldate = {2022-01-01},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Linzen, Tal and Dupoux, Emmanuel and Goldberg, Yoav},
	month = dec,
	year = {2016},
	pages = {521--535},
}

@article{erickson_rules_nodate,
	title = {Rules and {Exemplars} in {Category} {Learning}},
	language = {en},
	author = {Erickson, Michael A and Kruschke, John K},
	pages = {62},
}

@article{lake_word_2021,
	title = {Word meaning in minds and machines},
	url = {http://arxiv.org/abs/2008.01766},
	abstract = {Machines have achieved a broad and growing set of linguistic competencies, thanks to recent progress in Natural Language Processing (NLP). Psychologists have shown increasing interest in such models, comparing their output to psychological judgments such as similarity, association, priming, and comprehension, raising the question of whether the models could serve as psychological theories. In this article, we compare how humans and machines represent the meaning of words. We argue that contemporary NLP systems are fairly successful models of human word similarity, but they fall short in many other respects. Current models are too strongly linked to the text-based patterns in large corpora, and too weakly linked to the desires, goals, and beliefs that people express through words. Word meanings must also be grounded in perception and action and be capable of ﬂexible combinations in ways that current systems are not. We discuss more promising approaches to grounding NLP systems and argue that they will be more successful with a more human-like, conceptual basis for word meaning.},
	language = {en},
	urldate = {2021-12-20},
	journal = {arXiv:2008.01766 [cs]},
	author = {Lake, Brenden M. and Murphy, Gregory L.},
	month = apr,
	year = {2021},
	note = {arXiv: 2008.01766},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{nye_improving_nodate,
	title = {Improving {Coherence} and {Consistency} in {Neural} {Sequence} {Models} with {Dual}-{System}, {Neuro}-{Symbolic} {Reasoning}},
	abstract = {Human reasoning can be understood as an interplay between two systems: the intuitive and associative (“System 1”) and the deliberative and logical (“System 2”). Neural sequence models—which have been increasingly successful at performing complex, structured tasks—exhibit the advantages and failure modes of System 1: they are fast and learn patterns from data, but are often inconsistent and incoherent. In this work, we seek a lightweight, training-free means of improving existing System 1-like sequence models by adding System 2-inspired logical reasoning. We explore several variations on this theme in which candidate generations from a neural sequence model are examined for logical consistency by a symbolic reasoning module, which can either accept or reject the generations. Our approach uses neural inference to mediate between the neural System 1 and the logical System 2. Results in robust story generation and grounded instruction-following show that this approach can increase the coherence and accuracy of neurally-based generations.},
	language = {en},
	author = {Nye, Maxwell and Tessler, Michael Henry and Tenenbaum, Joshua B},
	pages = {13},
}

@article{orhan_self-supervised_2020,
	title = {Self-supervised learning through the eyes of a child},
	url = {http://arxiv.org/abs/2007.16189},
	abstract = {Within months of birth, children develop meaningful expectations about the world around them. How much of this early knowledge can be explained through generic learning mechanisms applied to sensory data, and how much of it requires more substantive innate inductive biases? Addressing this fundamental question in its full generality is currently infeasible, but we can hope to make real progress in more narrowly deﬁned domains, such as the development of high-level visual categories, thanks to improvements in data collecting technology and recent progress in deep learning. In this paper, our goal is precisely to achieve such progress by utilizing modern self-supervised deep learning methods and a recent longitudinal, egocentric video dataset recorded from the perspective of three young children (Sullivan et al., 2020). Our results demonstrate the emergence of powerful, high-level visual representations from developmentally realistic natural videos using generic selfsupervised learning objectives.},
	language = {en},
	urldate = {2021-12-19},
	journal = {arXiv:2007.16189 [cs]},
	author = {Orhan, A. Emin and Gupta, Vaibhav V. and Lake, Brenden M.},
	month = dec,
	year = {2020},
	note = {arXiv: 2007.16189},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{soares_matching_2019,
	title = {Matching the {Blanks}: {Distributional} {Similarity} for {Relation} {Learning}},
	shorttitle = {Matching the {Blanks}},
	url = {http://arxiv.org/abs/1906.03158},
	abstract = {General purpose relation extractors, which can model arbitrary relations, are a core aspiration in information extraction. Efforts have been made to build general purpose extractors that represent relations with their surface forms, or which jointly embed surface forms with relations from an existing knowledge graph. However, both of these approaches are limited in their ability to generalize. In this paper, we build on extensions of Harris' distributional hypothesis to relations, as well as recent advances in learning text representations (specifically, BERT), to build task agnostic relation representations solely from entity-linked text. We show that these representations significantly outperform previous work on exemplar based relation extraction (FewRel) even without using any of that task's training data. We also show that models initialized with our task agnostic representations, and then tuned on supervised relation extraction datasets, significantly outperform the previous methods on SemEval 2010 Task 8, KBP37, and TACRED.},
	urldate = {2021-12-19},
	journal = {arXiv:1906.03158 [cs]},
	author = {Soares, Livio Baldini and FitzGerald, Nicholas and Ling, Jeffrey and Kwiatkowski, Tom},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.03158},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@article{gelderloos_active_nodate,
	title = {Active {Word} {Learning} through {Self}-supervision},
	language = {en},
	author = {Gelderloos, Lieke and Kamelabad, Alireza Mahmoudi and Alishahi, Afra},
	pages = {7},
}

@techreport{vong_cross-situational_2021,
	title = {Cross-situational word learning with multimodal neural networks},
	url = {https://psyarxiv.com/udbh2/},
	abstract = {In order to learn the mappings from words to referents, children must integrate co-occurrence information across individually ambiguous pairs of scenes and utterances, a challenge known as cross-situational word learning. In machine learning, recent multimodal neural networks have been shown to learn meaningful visual-linguistic mappings from cross-situational data, as needed to solve problems such as image captioning and visual question answering. These networks are potentially appealing as cognitive models because they can learn from raw visual and linguistic stimuli, something previous cognitive models have not addressed. In this paper, we examine whether recent machine learning approaches can help explain various behavioral phenomena from the psychological literature on cross-situational word learning. We consider two variants of a multimodal neural network architecture, and look at seven different phenomena associated with cross-situational word learning, and word learning more generally. Our results show that these networks can learn word-referent mappings from a single epoch of training, matching the amount of training found in cross-situational word learning experiments. Additionally, these networks capture some, but not all of the phenomena we studied, with all of the failures related to reasoning via mutual exclusivity. These results provide insight into the kinds of phenomena that arise naturally from relatively generic neural network learning algorithms, and which word learning phenomena require additional inductive biases.},
	language = {en-us},
	urldate = {2021-12-19},
	institution = {PsyArXiv},
	author = {Vong, Wai Keen and Lake, Brenden M.},
	month = jun,
	year = {2021},
	doi = {10.31234/osf.io/udbh2},
	note = {type: article},
	keywords = {Cognitive Psychology, Language, Social and Behavioral Sciences, cross-situational word learning, multimodal neural networks, word learning},
}

@article{griths_bayesian_nodate,
	title = {Bayesian models of cognition},
	language = {en},
	journal = {BAYESIAN MODELS},
	author = {Griﬃths, Thomas L and Kemp, Charles and Tenenbaum, Joshua B},
	pages = {49},
}

@article{raji_ai_2021,
	title = {{AI} and the {Everything} in the {Whole} {Wide} {World} {Benchmark}},
	url = {http://arxiv.org/abs/2111.15366},
	abstract = {There is a tendency across different subfields in AI to valorize a small collection of influential benchmarks. These benchmarks operate as stand-ins for a range of anointed common problems that are frequently framed as foundational milestones on the path towards flexible and generalizable AI systems. State-of-the-art performance on these benchmarks is widely understood as indicative of progress towards these long-term goals. In this position paper, we explore the limits of such benchmarks in order to reveal the construct validity issues in their framing as the functionally "general" broad measures of progress they are set up to be.},
	urldate = {2021-12-18},
	journal = {arXiv:2111.15366 [cs]},
	author = {Raji, Inioluwa Deborah and Bender, Emily M. and Paullada, Amandalynne and Denton, Emily and Hanna, Alex},
	month = nov,
	year = {2021},
	note = {arXiv: 2111.15366},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Performance, READ},
}

@article{gao_pile_2020,
	title = {The {Pile}: {An} {800GB} {Dataset} of {Diverse} {Text} for {Language} {Modeling}},
	shorttitle = {The {Pile}},
	url = {http://arxiv.org/abs/2101.00027},
	abstract = {Recent work has demonstrated that increased training dataset diversity improves general cross-domain knowledge and downstream generalization capability for large-scale language models. With this in mind, we present {\textbackslash}textit\{the Pile\}: an 825 GiB English text corpus targeted at training large-scale language models. The Pile is constructed from 22 diverse high-quality subsets -- both existing and newly constructed -- many of which derive from academic or professional sources. Our evaluation of the untuned performance of GPT-2 and GPT-3 on the Pile shows that these models struggle on many of its components, such as academic writing. Conversely, models trained on the Pile improve significantly over both Raw CC and CC-100 on all components of the Pile, while improving performance on downstream evaluations. Through an in-depth exploratory analysis, we document potentially concerning aspects of the data for prospective users. We make publicly available the code used in its construction.},
	urldate = {2021-12-18},
	journal = {arXiv:2101.00027 [cs]},
	author = {Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and Presser, Shawn and Leahy, Connor},
	month = dec,
	year = {2020},
	note = {arXiv: 2101.00027},
	keywords = {Computer Science - Computation and Language, READ},
}

@article{holtzman_curious_2020,
	title = {The {Curious} {Case} of {Neural} {Text} {Degeneration}},
	url = {http://arxiv.org/abs/1904.09751},
	abstract = {Despite considerable advancements with deep neural language models, the enigma of neural text degeneration persists when these models are tested as text generators. The counter-intuitive empirical observation is that even though the use of likelihood as training objective leads to high quality models for a broad range of language understanding tasks, using likelihood as a decoding objective leads to text that is bland and strangely repetitive. In this paper, we reveal surprising distributional differences between human text and machine text. In addition, we find that decoding strategies alone can dramatically effect the quality of machine text, even when generated from exactly the same neural language model. Our findings motivate Nucleus Sampling, a simple but effective method to draw the best out of neural generation. By sampling text from the dynamic nucleus of the probability distribution, which allows for diversity while effectively truncating the less reliable tail of the distribution, the resulting text better demonstrates the quality of human text, yielding enhanced diversity without sacrificing fluency and coherence.},
	urldate = {2021-12-18},
	journal = {arXiv:1904.09751 [cs]},
	author = {Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell and Choi, Yejin},
	month = feb,
	year = {2020},
	note = {arXiv: 1904.09751},
	keywords = {Computer Science - Computation and Language, READ},
}

@article{borgeaud_improving_2021,
	title = {Improving language models by retrieving from trillions of tokens},
	url = {http://arxiv.org/abs/2112.04426},
	abstract = {We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a \$2\$ trillion token database, our Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25\${\textbackslash}times\$ fewer parameters. After fine-tuning, RETRO performance translates to downstream knowledge-intensive tasks such as question answering. RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train RETRO from scratch, yet can also rapidly RETROfit pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale.},
	language = {en},
	urldate = {2021-12-18},
	journal = {arXiv:2112.04426 [cs]},
	author = {Borgeaud, Sebastian and Mensch, Arthur and Hoffmann, Jordan and Cai, Trevor and Rutherford, Eliza and Millican, Katie and Driessche, George van den and Lespiau, Jean-Baptiste and Damoc, Bogdan and Clark, Aidan and Casas, Diego de Las and Guy, Aurelia and Menick, Jacob and Ring, Roman and Hennigan, Tom and Huang, Saffron and Maggiore, Loren and Jones, Chris and Cassirer, Albin and Brock, Andy and Paganini, Michela and Irving, Geoffrey and Vinyals, Oriol and Osindero, Simon and Simonyan, Karen and Rae, Jack W. and Elsen, Erich and Sifre, Laurent},
	month = dec,
	year = {2021},
	note = {arXiv: 2112.04426},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, READ},
}

@article{khandelwal_sharp_2018,
	title = {Sharp {Nearby}, {Fuzzy} {Far} {Away}: {How} {Neural} {Language} {Models} {Use} {Context}},
	shorttitle = {Sharp {Nearby}, {Fuzzy} {Far} {Away}},
	url = {http://arxiv.org/abs/1805.04623},
	abstract = {We know very little about how neural language models (LM) use prior linguistic context. In this paper, we investigate the role of context in an LSTM LM, through ablation studies. Speciﬁcally, we analyze the increase in perplexity when prior context words are shufﬂed, replaced, or dropped. On two standard datasets, Penn Treebank and WikiText-2, we ﬁnd that the model is capable of using about 200 tokens of context on average, but sharply distinguishes nearby context (recent 50 tokens) from the distant history. The model is highly sensitive to the order of words within the most recent sentence, but ignores word order in the long-range context (beyond 50 tokens), suggesting the distant past is modeled only as a rough semantic ﬁeld or topic. We further ﬁnd that the neural caching model (Grave et al., 2017b) especially helps the LSTM to copy words from within this distant context. Overall, our analysis not only provides a better understanding of how neural LMs use their context, but also sheds light on recent success from cache-based models.},
	language = {en},
	urldate = {2021-12-18},
	journal = {arXiv:1805.04623 [cs]},
	author = {Khandelwal, Urvashi and He, He and Qi, Peng and Jurafsky, Dan},
	month = may,
	year = {2018},
	note = {arXiv: 1805.04623},
	keywords = {Computer Science - Computation and Language},
}

@article{baddeley_working_2003,
	title = {Working memory: looking back and looking forward},
	volume = {4},
	issn = {1471-003X, 1471-0048},
	shorttitle = {Working memory},
	url = {http://www.nature.com/articles/nrn1201},
	doi = {10.1038/nrn1201},
	language = {en},
	number = {10},
	urldate = {2021-12-18},
	journal = {Nature Reviews Neuroscience},
	author = {Baddeley, Alan},
	month = oct,
	year = {2003},
	pages = {829--839},
}

@misc{noauthor_zotero_nodate,
	title = {Zotero {\textbar} {Your} personal research assistant},
	url = {https://www.zotero.org/start},
	urldate = {2021-12-18},
}
